{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "pygments_lexer": "ipython2", 
            "file_extension": ".py", 
            "name": "python", 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "execution_count": 1, 
            "cell_type": "code", 
            "source": "# Binary Classification with Sonar Dataset: Standardized\nimport numpy\nfrom pandas import read_csv\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline", 
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "Using TensorFlow backend.\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "execution_count": 2, 
            "cell_type": "code", 
            "source": "!rm -f sonar.csv\n!wget https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data -O sonar.csv", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "--2018-02-02 04:14:21--  https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\nResolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\nConnecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 87776 (86K) [text/plain]\nSaving to: \u2018sonar.csv\u2019\n\n100%[======================================>] 87,776      --.-K/s   in 0.1s    \n\n2018-02-02 04:14:22 (666 KB/s) - \u2018sonar.csv\u2019 saved [87776/87776]\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "execution_count": 3, 
            "cell_type": "code", 
            "source": "# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": 4, 
            "cell_type": "code", 
            "source": "# load dataset\ndataframe = read_csv(\"sonar.csv\", header=None)\ndataset = dataframe.values\n# split into input and output variables\nX = dataset[:,0:60].astype(float)\nY = dataset[:,60]", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": 7, 
            "cell_type": "code", 
            "source": "# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": 9, 
            "cell_type": "code", 
            "source": "# baseline model\ndef create_baseline():\n    # create model\n    model = Sequential()\n    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": 10, 
            "cell_type": "code", 
            "source": "# evaluate model with standardized dataset\nestimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(estimator, X, encoded_Y, cv=kfold)\nprint(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n\n# w/o standardization", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Baseline: 80.28% (4.66%)\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "execution_count": 11, 
            "cell_type": "code", 
            "source": "estimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100,batch_size=5, verbose=0)))\n\npipeline = Pipeline(estimators)\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\nprint(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n\n# with standardization \n# using pipelines.. and standardizers", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Standardized: 85.09% (5.87%)\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "execution_count": 12, 
            "cell_type": "code", 
            "source": "#Lo behold.. a vast increase in the performance..", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "    An aspect that may have an outsized effect is the\n    structure of the network itself called the network topology. In this section we take a look at two\n    experiments on the structure of the network: making it smaller and making it larger."
        }, 
        {
            "metadata": {}, 
            "execution_count": 13, 
            "cell_type": "code", 
            "source": "# smaller model\ndef create_smaller():\n    # create model\n    model = Sequential()\n    model.add(Dense(30, input_dim=60, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100,batch_size=5, verbose=0)))\n\npipeline = Pipeline(estimators)\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\nprint(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Smaller: 86.06% (6.95%)\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "execution_count": 14, 
            "cell_type": "code", 
            "source": "def create_larger():\n    # create model\n    model = Sequential()\n    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=100, batch_size=5,verbose=0)))\n\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\nprint(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Larger: 84.59% (7.74%)\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "execution_count": 15, 
            "cell_type": "code", 
            "source": "# a smaller model gave a better accuracy.. slightly.. albeit\n# a larget one.. has a dip..\n# could be statistical noise..\n# or a call for more training.. of the network..", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 1
}