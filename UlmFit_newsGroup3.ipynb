{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UlmFit_newsGroup3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronykroy/DNN-NLP-and-other-stuff/blob/master/UlmFit_newsGroup3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN-c0d1Wbone",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3is the base line 4 is deeper training\n",
        "# based on https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson3-imdb.ipynb\n",
        "# but usgin newsgroups data..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtoluY-3XIWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3sW77aLXAjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fastai\n",
        "from fastai import *\n",
        "from fastai.text import * \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "import io\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnnjWjFpXKOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZAiOxKjXaPh",
        "colab_type": "code",
        "outputId": "09f1f415-779d-43ee-c943-066133e0cd1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(documents)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOx8QifIXd15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({'label':dataset.target, 'text':dataset.data})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imuBy_qwptXg",
        "colab_type": "code",
        "outputId": "626af104-b325-4366-eeb9-179a4e526c21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f_JnhOWpmnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32faljnbpvhe",
        "colab_type": "code",
        "outputId": "925dbf18-e885-43d1-a952-f654989cef66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc8XKu-wae7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_d  = pathlib.PosixPath('/sample_data2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C3NBiATw_DC",
        "colab_type": "code",
        "outputId": "b9eebde3-68fb-4c77-cc09-23afeb540c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.path.exists(path_d)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQgheLlxxBTG",
        "colab_type": "code",
        "outputId": "d6ed3821-bd85-4404-f09c-86c7c57dac51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path_d"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/sample_data2')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT-VkljhhyV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(path_d):\n",
        "    os.mkdir(path_d) # if the path doesnt exist make it..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pse9KdrHhcXE",
        "colab_type": "code",
        "outputId": "6cf5bc6d-0c55-4850-ca8b-09ffd58083bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path_d/'texts.csv'"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/sample_data2/texts.csv')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsFGzqDciaEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.rename({'label':'target','text':text},inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0_6NKfvZ3b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(path_d/'texts.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79_hPn4Bi42l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs=16# reduced from 48"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFmNXF7LperX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmzyC55cs97i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_trn, df_val = train_test_split(df, stratify = df['label'], test_size = 0.4, random_state = 12) # test_size is name sake\n",
        "# test train split gives you just that test and train\n",
        "# use twice.. 2nd time on train to get a separate test nd val set..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PYKcu7DXoXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm = TextLMDataBunch.from_df(train_df = df_trn, valid_df = df_val, path = path_d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPsuE3nwYNu1",
        "colab_type": "code",
        "outputId": "27505e9a-dac6-4cdc-8233-7387799c94c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "data_lm.save( path_d/'data_lm.pkl') # saving as a back stop"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CrossEntropyLoss. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khxbyXAcwlzR",
        "colab_type": "code",
        "outputId": "65aff43e-9f89-4a15-9e8d-3b5d2736c142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls /sample_data2/"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_lm.pkl  \u001b[0m\u001b[01;34mmodels\u001b[0m/  texts.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyKet0ZatiiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm = load_data( '/sample_data2','data_lm.pkl', bs=bs) # jesus... give the path and the file in to which this was saved earlier.. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65QLsCDRYOQw",
        "colab_type": "code",
        "outputId": "66ba6dd6-e3b2-4415-ab52-efe88e749bab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "data_lm.show_batch()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>abuse \" and such , or course . \\n  xxmaj nobody seems to have noticed that the xxmaj treasury department has nothing \\n  to do with sex crimes . xxmaj or maybe the feds have recently instituted a \\n  xxup tax on sex crimes ... xxmaj yeah , that 's why the xxup batf was there , looking for \\n  unregistered * guns * ( \"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>forward essentially . \\n \\t \\n \\t  xxmaj but in terms of what is coming out of this weekend , \\n  i do n't yet know . xxmaj in a couple hours we 'll know . \\n \\t \\n \\t  q ? \\t  xxmaj just a follow - up on the financing here . xxmaj is \\n  any of this robbing xxmaj peter to pay</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>have helped a little \\n  and good luck with the _ xxmaj oxygen sensor _ ? \\n \\n \\t\\t\\t\\t - xxmaj thomas - xxbos xxmaj for those of you interested , i just finished talking with a \\n  representative of xxmaj senator xxmaj bob xxmaj krueger 's xxunk campaign about his \\n  position on the xxup rtkba . xxmaj krueger was appointed by the xxmaj democratic xxmaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>project on xxunk performance , and have a \\n  wealth of data on particle size and output which we are going to use \\n  to xxunk a contract next week . \\n \\n  xxmaj although the output data is easy for us to present , there seems to be \\n  little concensus on the optimum diameter of the xxunk droplets for \\n  straightforward inhalation therapy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>- 870 - 0300 ) include a multi - font xxup xxunk document composer ; for several \\n  systems . \\n \\n \\t arbortext , xxmaj inc. provides an xxup x11 version of its xxmaj electronic xxmaj publishing \\n  program called \" xxmaj the xxmaj publisher \" . xxmaj the xxmaj publisher is available on xxmaj sun , xxup hp and \\n  xxmaj apollo workstations . xxmaj</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb5qlPynYPZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.7) # was 0.3\n",
        "# drop_mult ::hyper-parameter \n",
        "# used for regularization, sets the amount of dropout{duh}\n",
        "#If the model is over-fitting increase it, if under-fitting, you can decrease the number{again duh} but just a reminder if you are confounded by it again\n",
        "# source:: https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7KaRyQ-YQmI",
        "colab_type": "code",
        "outputId": "816444c6-e4e4-4527-90b0-823d60d435ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9_1I7xaaF42",
        "colab_type": "code",
        "outputId": "7e0a6454-1447-42ac-9d33-8921fbe17ac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "learn.recorder.plot(skip_end=15)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEKCAYAAAAvlUMdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHOO5qWcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOlRvNPFaIVU",
        "colab_type": "code",
        "outputId": "aca414fa-5c35-4a80-955b-ca09925c4b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "learn.fit_one_cycle(3, 1e-2, moms=(0.8,0.7)) # 1e-2 works fair enough as usual\n",
        "# theoretically you keep training for as long as the loss keeps decreasing...\n",
        "# the accuracy's [or any other metric] calcualtion is done to show where your model stands after that epoch\n",
        "#  overfitting udnerfitting is a concern.. when you are doing a lot of epochs.. for a few epochs go by the above"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.385291</td>\n",
              "      <td>4.233065</td>\n",
              "      <td>0.270084</td>\n",
              "      <td>09:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.544013</td>\n",
              "      <td>4.118543</td>\n",
              "      <td>0.278826</td>\n",
              "      <td>09:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.285003</td>\n",
              "      <td>4.020329</td>\n",
              "      <td>0.292102</td>\n",
              "      <td>09:39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wSzjXCo0I5k",
        "colab_type": "text"
      },
      "source": [
        "For a language model the accuracy being low is fine.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkPe_fULup7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('fit_head') # fit_head nomenclature: etymology: because we trained only for th last layer.. head of the model...?maybe so"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp-9-J7NurlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('fit_head');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pSxeSx-us5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.unfreeze() # unfreeze all layers to compete the training of full model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGLkeVUjwiPm",
        "colab_type": "code",
        "outputId": "f3819ae5-d849-4470-bbf2-4be8fa1ac785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "learn.fit_one_cycle(6, 1e-3, moms=(0.8,0.7)) # originally 10 cycles but we dont have time :)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.791799</td>\n",
              "      <td>3.842836</td>\n",
              "      <td>0.317264</td>\n",
              "      <td>10:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.743700</td>\n",
              "      <td>3.821657</td>\n",
              "      <td>0.326245</td>\n",
              "      <td>10:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.570505</td>\n",
              "      <td>3.724357</td>\n",
              "      <td>0.342541</td>\n",
              "      <td>10:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.482038</td>\n",
              "      <td>3.674882</td>\n",
              "      <td>0.351830</td>\n",
              "      <td>11:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.350525</td>\n",
              "      <td>3.643360</td>\n",
              "      <td>0.357335</td>\n",
              "      <td>11:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.077419</td>\n",
              "      <td>3.649883</td>\n",
              "      <td>0.357677</td>\n",
              "      <td>11:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrWK-8bv7DQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.fit_one_cycle(2, 1e-3, moms=(0.8,0.7)) # lets do two more because we are feeling generous :P"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09iBIDOHwqnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('fine_tuned') # save # stop gap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQFzVeb4wsIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('fine_tuned');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWjoRXTAwtfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = \"and today\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2 # test how good the model is at predicting the next few words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFpqJFomwySW",
        "colab_type": "code",
        "outputId": "047efcc5-ecfe-496b-8419-f0669c880433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "and today they 're in the process of iraqi control over the \n",
            "  Fbi and other criminals . \n",
            " \n",
            "  The Fbi , who are now going to be there , will be the \n",
            "  FBI 's .\n",
            "and today i got a very good news message which developed a \n",
            "  simple way to make the Clipper algorithm work , but it ca n't \n",
            "  be easily realized by anyone . i have n't heard of any decent \n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQhObiecwzyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save_encoder('fine_tuned_enc') # we need the encoder in particular.. not exactly the model that predicts next word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOzGi3dcX6YN",
        "colab_type": "text"
      },
      "source": [
        "### The Actual Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyjOp2IzE0MB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifier model data\n",
        "data_clas = TextClasDataBunch.from_df(path = path_d, train_df = df_trn, valid_df = df_val, vocab=data_lm.train_ds.vocab, bs=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt9KG8sSEnjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5) # the actual classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuBQhtt0FZr0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "44f3244f-8eff-495e-c642-6aaa94748d47"
      },
      "source": [
        "learn.load_encoder('fine_tuned_enc')"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (6788 items)\n",
              "x: TextList\n",
              "xxbos \n",
              " \n",
              "  xxup cnn just claimed he bought 104 \" semi - automatic assault rifles \" . xxmaj and \n",
              "  they say xxmaj koresh was n't god - like ... xxmaj he managed to buy or build a \n",
              "  collection of fully - automatic semi - automatic rifles ... xxmaj quite a feat , \n",
              "  i would say . ;-) \n",
              " \n",
              "  xxmaj they 're still making charges of \" sexual abuse \" and such , or course . \n",
              "  xxmaj nobody seems to have noticed that the xxmaj treasury department has nothing \n",
              "  to do with sex crimes . xxmaj or maybe the feds have recently instituted a \n",
              "  xxup tax on sex crimes ... xxmaj yeah , that 's why the xxup batf was there , looking for \n",
              "  unregistered * guns * ( \" this is my weapon , this is my gun , this is for \n",
              "  fighting , this is for ... \" ) . \n",
              " \n",
              " \n",
              "  i also heard that they 're claiming to be cautious because of xxmaj koresh 's \n",
              "  \" heated ammunition stockpile \" . i seem to recall that smokeless powder \n",
              "  tends to decompose at even moderate temperatures . i would be rather \n",
              "  surprised , after a fire of that nature , if * any * of his \" stockpile \" is \n",
              "  xxunk , or xxunk . \n",
              " \n",
              " \n",
              "  i seem to recall that aluminum powder is a common component of \n",
              "  xxunk ... xxmaj the folks on xxunk could probably tell you . \n",
              " \n",
              " \n",
              "  i think * anything * is legal if you have the proper license . xxmaj if he had \n",
              "  a \" xxunk and relics \" permit , i believe he could legally own \n",
              "  xxunk to go with his launcher . \n",
              " \n",
              "  -- \n",
              "  xxmaj charles xxmaj scripter * cescript@phy.mtu.edu \n",
              "  xxmaj dept of xxmaj physics , xxmaj michigan xxmaj tech , xxmaj houghton , xxup mi 49931,xxbos xxmaj hi , \n",
              " \t i am looking for some help in choosing a package \n",
              "  for a high - speed silicon xxup adc ( 100mhz ) currently being \n",
              "  fabricated . xxmaj this is a phd research project and i have to test \n",
              "  the chip at speed on a xxup pcb . i expect to have roughly 100 \n",
              "  packaged circuits and will do xxup dc , low - speed and high - speed \n",
              "  testing using 3 different set - ups for the test chip . \n",
              " \t \n",
              " \t i know for sure that a xxup dip will not work \n",
              "  ( the long lead lines have too high an inductance ) . \n",
              "  xxmaj getting a custom - made package is too expensive , so \n",
              "  i am trying to choose between a xxunk and a \n",
              "  leadless chip carrier . xxmaj the xxunk would be hard \n",
              "  to test since it has to be soldered on to the test \n",
              "  setup and i would spend loads of time soldering \n",
              "  as i kept changing the test chip . xxmaj the leadless chip \n",
              "  carrier sockets also have long lead lines and may \n",
              "  not work at high speeds . \n",
              " \n",
              " \t xxmaj does anyone out there have experience / knowledge \n",
              "  of this field ? i would greatly appreciate help ! xxmaj any ideas / \n",
              "  names of companies manufacturing holders / sockets / packages \n",
              "  would help . \n",
              " \n",
              "  xxup p.s. xxmaj the multi - layer fancy gaas packages seem like a bit \n",
              "  of overkill ( ? ),xxbos \n",
              "  i really do n't understand all this ! i watched on satellite network feeds as \n",
              "  perhaps 90 people died before my eyes , while the two xxmaj xxunk 's fanned the flames , \n",
              "  and the xxup fbi stopped the xxunk at the gate . \n",
              " \n",
              "  xxmaj something was xxup very wrong with that scene . \n",
              " \n",
              "  xxmaj perhaps if i 'd watched xxup rambo movies , i might 've been xxunk to the pain of \n",
              "  fellow humans dying . \n",
              " \n",
              "  xxmaj thank xxup god i still feel . i 'm very sorry for you who do n't . xxmaj for you who think \n",
              "  they got what they deserved . xxmaj can you really believe that ? xxmaj even if xxmaj koresh was \n",
              "  the xxunk mad man they said he was , did the others deserve his fate ? xxmaj if , \n",
              "  in fact , he was mad , was n't that even more reason to believe he duped his \n",
              "  followers , and therefore they were innocent , brainwashed , victims ? xxmaj is there \n",
              "  any xxunk that justifies all that death ? \n",
              " \n",
              "  xxmaj and if not , it is clear that the deaths would not have occured if the xxup batf has \n",
              "  not xxup fucked xxup up initially , and now the xxup fbi got xxunk and pushed xxmaj xxunk over \n",
              "  the edge . \n",
              " \n",
              "  xxmaj and that 's if you buy the latest version of the \" story \" hook , line , and xxunk . \n",
              "  i have believed all along that they could not let them live , the embarrassment \n",
              "  to the xxup batf and the xxup fbi would 've been too severe . \n",
              " \n",
              "  xxmaj remember , this was a suspicion of tax - evasion warrant . xxmaj there were no \n",
              "  witnesses , except the xxup fbi . xxmaj all information filtered through the xxup fbi . xxmaj all they \n",
              "  had to do was allow one remote controlled pool camera be installed near the \n",
              "  building , and the press could 've done their job , and would 've been able to back \n",
              "  the xxup fbi 's story with close up video , while xxunk no risk to the press . \n",
              "  xxmaj unless they did not want the public to see something . xxmaj the complete lack of any \n",
              "  other source of information other than the xxup fbi really causes me concern . \n",
              " \n",
              " \n",
              "  xxmaj sick to my stomach , and getting xxunk from all the xxmaj government apologists \n",
              "  -- \n",
              "  jmd@handheld.com,xxbos xxmaj however , this has nothing to do with motorcycling , unless you consider \n",
              "  the xxmaj xxunk a bike .,xxbos i have a novell xxunk that i will sell for $ 692 which can be upgraded to 3.11 \n",
              "  for $ 460 . xxmaj the novell has complete documentation but no network cards except \n",
              "  the xxup id card . \n",
              " \n",
              " \n",
              "  --\n",
              "y: CategoryList\n",
              "16,12,16,8,6\n",
              "Path: /sample_data2;\n",
              "\n",
              "Valid: LabelList (4526 items)\n",
              "x: TextList\n",
              "xxbos \n",
              " \n",
              " \n",
              "  xxmaj and still we wonder why they stereotype us ...,xxbos \n",
              "  i 've been a member of the xxup nra for several years and recently \" joined \" \n",
              "  xxup hci . i wanted to see what they were up to and paid the minimum ( $ 15 ) \n",
              "  to get a membership . i also sent the xxup nra another $ 120 . \n",
              " ,xxbos i have n't seen much info about how to add an extra internal disk to a \n",
              "  mac . xxmaj we would like to try it , and i wonder if someone had some good \n",
              "  advice . \n",
              " \n",
              "  xxmaj we have a xxmaj mac iicx with the original internal xxmaj quantum 40 xxup mb hard disk , \n",
              "  and an unusable floppy drive . xxmaj we also have a new spare xxmaj connor 40 xxup mb \n",
              "  disk which we would like to use . xxmaj the idea is to replace the broken \n",
              "  floppy drive with the new hard disk , but there seems to be some \n",
              "  problems : \n",
              " \n",
              "  xxmaj the internal xxup scsi cable and power cable inside the cx has only \n",
              "  connectors for one single hard disk drive . \n",
              " \n",
              "  xxmaj if i made a ribbon cable and a power cable with three connectors each \n",
              "  ( 1 for motherboard , 1 for each of the 2 disks ) , would it work ? \n",
              " \n",
              "  xxmaj is the iicx able to supply the extra power to the extra disk ? \n",
              " \n",
              "  xxmaj what about xxunk ? i suppose that i should remove the resistor \n",
              "  packs from the disk that is closest to the motherboard , but leave them \n",
              "  installed in the other disk . \n",
              " \n",
              "  xxmaj the xxup scsi xxup id jumpers should also be changed so that the new disk gets \n",
              "  xxup id # 1 . xxmaj the old one should have xxup id # 0 . \n",
              " \n",
              "  xxmaj it is no problem for us to remove the floppy drive , as we have an \n",
              "  external floppy that we can use if it wo n't boot of the hard disk . \n",
              " \n",
              "  xxmaj thank you ! \n",
              " ,xxbos \t  xxrep 6 ^ what the hell xxunk a ' xxunk ' ? ? ( xxunk ( sp ) ) ? ? \n",
              " ,xxbos a question regarding the xxmaj islamic view towards homosexuality came up in a \n",
              "  discussion group that i participate in , and i 'd like to ask the question here , \n",
              " \n",
              "  \" xxmaj what is the xxmaj islamic view towards homosexuality ? \"\n",
              "y: CategoryList\n",
              "8,16,4,6,19\n",
              "Path: /sample_data2;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(33120, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(33120, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f66ddb72ea0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/sample_data2'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (6788 items)\n",
              "x: TextList\n",
              "xxbos \n",
              " \n",
              "  xxup cnn just claimed he bought 104 \" semi - automatic assault rifles \" . xxmaj and \n",
              "  they say xxmaj koresh was n't god - like ... xxmaj he managed to buy or build a \n",
              "  collection of fully - automatic semi - automatic rifles ... xxmaj quite a feat , \n",
              "  i would say . ;-) \n",
              " \n",
              "  xxmaj they 're still making charges of \" sexual abuse \" and such , or course . \n",
              "  xxmaj nobody seems to have noticed that the xxmaj treasury department has nothing \n",
              "  to do with sex crimes . xxmaj or maybe the feds have recently instituted a \n",
              "  xxup tax on sex crimes ... xxmaj yeah , that 's why the xxup batf was there , looking for \n",
              "  unregistered * guns * ( \" this is my weapon , this is my gun , this is for \n",
              "  fighting , this is for ... \" ) . \n",
              " \n",
              " \n",
              "  i also heard that they 're claiming to be cautious because of xxmaj koresh 's \n",
              "  \" heated ammunition stockpile \" . i seem to recall that smokeless powder \n",
              "  tends to decompose at even moderate temperatures . i would be rather \n",
              "  surprised , after a fire of that nature , if * any * of his \" stockpile \" is \n",
              "  xxunk , or xxunk . \n",
              " \n",
              " \n",
              "  i seem to recall that aluminum powder is a common component of \n",
              "  xxunk ... xxmaj the folks on xxunk could probably tell you . \n",
              " \n",
              " \n",
              "  i think * anything * is legal if you have the proper license . xxmaj if he had \n",
              "  a \" xxunk and relics \" permit , i believe he could legally own \n",
              "  xxunk to go with his launcher . \n",
              " \n",
              "  -- \n",
              "  xxmaj charles xxmaj scripter * cescript@phy.mtu.edu \n",
              "  xxmaj dept of xxmaj physics , xxmaj michigan xxmaj tech , xxmaj houghton , xxup mi 49931,xxbos xxmaj hi , \n",
              " \t i am looking for some help in choosing a package \n",
              "  for a high - speed silicon xxup adc ( 100mhz ) currently being \n",
              "  fabricated . xxmaj this is a phd research project and i have to test \n",
              "  the chip at speed on a xxup pcb . i expect to have roughly 100 \n",
              "  packaged circuits and will do xxup dc , low - speed and high - speed \n",
              "  testing using 3 different set - ups for the test chip . \n",
              " \t \n",
              " \t i know for sure that a xxup dip will not work \n",
              "  ( the long lead lines have too high an inductance ) . \n",
              "  xxmaj getting a custom - made package is too expensive , so \n",
              "  i am trying to choose between a xxunk and a \n",
              "  leadless chip carrier . xxmaj the xxunk would be hard \n",
              "  to test since it has to be soldered on to the test \n",
              "  setup and i would spend loads of time soldering \n",
              "  as i kept changing the test chip . xxmaj the leadless chip \n",
              "  carrier sockets also have long lead lines and may \n",
              "  not work at high speeds . \n",
              " \n",
              " \t xxmaj does anyone out there have experience / knowledge \n",
              "  of this field ? i would greatly appreciate help ! xxmaj any ideas / \n",
              "  names of companies manufacturing holders / sockets / packages \n",
              "  would help . \n",
              " \n",
              "  xxup p.s. xxmaj the multi - layer fancy gaas packages seem like a bit \n",
              "  of overkill ( ? ),xxbos \n",
              "  i really do n't understand all this ! i watched on satellite network feeds as \n",
              "  perhaps 90 people died before my eyes , while the two xxmaj xxunk 's fanned the flames , \n",
              "  and the xxup fbi stopped the xxunk at the gate . \n",
              " \n",
              "  xxmaj something was xxup very wrong with that scene . \n",
              " \n",
              "  xxmaj perhaps if i 'd watched xxup rambo movies , i might 've been xxunk to the pain of \n",
              "  fellow humans dying . \n",
              " \n",
              "  xxmaj thank xxup god i still feel . i 'm very sorry for you who do n't . xxmaj for you who think \n",
              "  they got what they deserved . xxmaj can you really believe that ? xxmaj even if xxmaj koresh was \n",
              "  the xxunk mad man they said he was , did the others deserve his fate ? xxmaj if , \n",
              "  in fact , he was mad , was n't that even more reason to believe he duped his \n",
              "  followers , and therefore they were innocent , brainwashed , victims ? xxmaj is there \n",
              "  any xxunk that justifies all that death ? \n",
              " \n",
              "  xxmaj and if not , it is clear that the deaths would not have occured if the xxup batf has \n",
              "  not xxup fucked xxup up initially , and now the xxup fbi got xxunk and pushed xxmaj xxunk over \n",
              "  the edge . \n",
              " \n",
              "  xxmaj and that 's if you buy the latest version of the \" story \" hook , line , and xxunk . \n",
              "  i have believed all along that they could not let them live , the embarrassment \n",
              "  to the xxup batf and the xxup fbi would 've been too severe . \n",
              " \n",
              "  xxmaj remember , this was a suspicion of tax - evasion warrant . xxmaj there were no \n",
              "  witnesses , except the xxup fbi . xxmaj all information filtered through the xxup fbi . xxmaj all they \n",
              "  had to do was allow one remote controlled pool camera be installed near the \n",
              "  building , and the press could 've done their job , and would 've been able to back \n",
              "  the xxup fbi 's story with close up video , while xxunk no risk to the press . \n",
              "  xxmaj unless they did not want the public to see something . xxmaj the complete lack of any \n",
              "  other source of information other than the xxup fbi really causes me concern . \n",
              " \n",
              " \n",
              "  xxmaj sick to my stomach , and getting xxunk from all the xxmaj government apologists \n",
              "  -- \n",
              "  jmd@handheld.com,xxbos xxmaj however , this has nothing to do with motorcycling , unless you consider \n",
              "  the xxmaj xxunk a bike .,xxbos i have a novell xxunk that i will sell for $ 692 which can be upgraded to 3.11 \n",
              "  for $ 460 . xxmaj the novell has complete documentation but no network cards except \n",
              "  the xxup id card . \n",
              " \n",
              " \n",
              "  --\n",
              "y: CategoryList\n",
              "16,12,16,8,6\n",
              "Path: /sample_data2;\n",
              "\n",
              "Valid: LabelList (4526 items)\n",
              "x: TextList\n",
              "xxbos \n",
              " \n",
              " \n",
              "  xxmaj and still we wonder why they stereotype us ...,xxbos \n",
              "  i 've been a member of the xxup nra for several years and recently \" joined \" \n",
              "  xxup hci . i wanted to see what they were up to and paid the minimum ( $ 15 ) \n",
              "  to get a membership . i also sent the xxup nra another $ 120 . \n",
              " ,xxbos i have n't seen much info about how to add an extra internal disk to a \n",
              "  mac . xxmaj we would like to try it , and i wonder if someone had some good \n",
              "  advice . \n",
              " \n",
              "  xxmaj we have a xxmaj mac iicx with the original internal xxmaj quantum 40 xxup mb hard disk , \n",
              "  and an unusable floppy drive . xxmaj we also have a new spare xxmaj connor 40 xxup mb \n",
              "  disk which we would like to use . xxmaj the idea is to replace the broken \n",
              "  floppy drive with the new hard disk , but there seems to be some \n",
              "  problems : \n",
              " \n",
              "  xxmaj the internal xxup scsi cable and power cable inside the cx has only \n",
              "  connectors for one single hard disk drive . \n",
              " \n",
              "  xxmaj if i made a ribbon cable and a power cable with three connectors each \n",
              "  ( 1 for motherboard , 1 for each of the 2 disks ) , would it work ? \n",
              " \n",
              "  xxmaj is the iicx able to supply the extra power to the extra disk ? \n",
              " \n",
              "  xxmaj what about xxunk ? i suppose that i should remove the resistor \n",
              "  packs from the disk that is closest to the motherboard , but leave them \n",
              "  installed in the other disk . \n",
              " \n",
              "  xxmaj the xxup scsi xxup id jumpers should also be changed so that the new disk gets \n",
              "  xxup id # 1 . xxmaj the old one should have xxup id # 0 . \n",
              " \n",
              "  xxmaj it is no problem for us to remove the floppy drive , as we have an \n",
              "  external floppy that we can use if it wo n't boot of the hard disk . \n",
              " \n",
              "  xxmaj thank you ! \n",
              " ,xxbos \t  xxrep 6 ^ what the hell xxunk a ' xxunk ' ? ? ( xxunk ( sp ) ) ? ? \n",
              " ,xxbos a question regarding the xxmaj islamic view towards homosexuality came up in a \n",
              "  discussion group that i participate in , and i 'd like to ask the question here , \n",
              " \n",
              "  \" xxmaj what is the xxmaj islamic view towards homosexuality ? \"\n",
              "y: CategoryList\n",
              "8,16,4,6,19\n",
              "Path: /sample_data2;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(33120, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(33120, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f66ddb72ea0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/sample_data2'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Embedding(33120, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(33120, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(33120, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(33120, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF9-_oOhGGQD",
        "colab_type": "code",
        "outputId": "9500f1fd-ba9a-422c-c8ab-359e2720e5d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atI5lUqzGign",
        "colab_type": "code",
        "outputId": "a819d0b1-78c4-45ea-ab89-01cb876f8194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "learn.recorder.plot()"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nttGAjwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVa5YbFCGzkz",
        "colab_type": "code",
        "outputId": "18487bf2-8d8a-48dd-9b0c-cba6421204a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "#list(range(1e-2,1e-1,10)) # range does only int\n",
        "import numpy as np\n",
        "for i in np.arange(1e-2,1e-1,1e-2):\n",
        "  print(i);"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.01\n",
            "0.02\n",
            "0.03\n",
            "0.04\n",
            "0.05\n",
            "0.060000000000000005\n",
            "0.06999999999999999\n",
            "0.08\n",
            "0.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dno8wWgCFZms",
        "colab_type": "code",
        "outputId": "bc9578a3-da3a-4f2e-f883-ec5f0413fc62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "learn.fit_one_cycle(1, 5e-2, moms=(0.8,0.7)) #was 1e-2 updated based on graph above"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.446844</td>\n",
              "      <td>1.233551</td>\n",
              "      <td>0.610031</td>\n",
              "      <td>07:29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LcKNYTeIkqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('first')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe2EemEYIluN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('first'); # save and load the state where the last layer alone is what has been retrained.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jStsmgGTikV",
        "colab_type": "code",
        "outputId": "9b899d76-5855-4101-ef9b-c44c7f38878b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "slice(1e-2/(2.6**4),1e-2) # refer lecture for how that 2.6**4 was arrived at... apparently its emperically arrived at"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "slice(0.00021882987290360977, 0.01, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ESAxaca8VBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "outputId": "615d179c-fc64-49e0-f227-a0a7f227393e"
      },
      "source": [
        "from fastai.utils.show_install import show_install\n",
        "show_install(1)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "```text\n",
            "=== Software === \n",
            "python        : 3.6.8\n",
            "fastai        : 1.0.58\n",
            "fastprogress  : 0.1.21\n",
            "torch         : 1.3.0+cu100\n",
            "nvidia driver : 418.67\n",
            "torch cuda    : 10.0.130 / is available\n",
            "torch cudnn   : 7603 / is enabled\n",
            "\n",
            "=== Hardware === \n",
            "nvidia gpus   : 1\n",
            "torch devices : 1\n",
            "  - gpu0      : 11441MB | Tesla K80\n",
            "\n",
            "=== Environment === \n",
            "platform      : Linux-4.14.137+-x86_64-with-Ubuntu-18.04-bionic\n",
            "distro        : #1 SMP Thu Aug 8 02:47:02 PDT 2019\n",
            "conda env     : Unknown\n",
            "python        : /usr/bin/python3\n",
            "sys.path      : \n",
            "/env/python\n",
            "/usr/lib/python36.zip\n",
            "/usr/lib/python3.6\n",
            "/usr/lib/python3.6/lib-dynload\n",
            "/usr/local/lib/python3.6/dist-packages\n",
            "/usr/lib/python3/dist-packages\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/extensions\n",
            "/root/.ipython\n",
            "\n",
            "Sun Oct 20 16:12:56 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0    57W / 149W |   3950MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n",
            "```\n",
            "\n",
            "Please make sure to include opening/closing ``` when you paste into forums/github to make the reports appear formatted as code sections.\n",
            "\n",
            "Optional package(s) to enhance the diagnostics can be installed with:\n",
            "pip install distro\n",
            "Once installed, re-run this utility to get the additional information\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7yJqNpUVXUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/fastai/fastai/issues/1383"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQiClwYK_2Gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.freeze_to(-2) # unfreeze all but last 2 layers\n",
        "#learn.fit_one_cycle(2, 1e-2, moms=(0.8,0.7))# and then train them\n",
        "# BLUNDER : you do not train them at the same rate\n",
        "# instead you train them using a much lower RL as below \n",
        "# if you use the same lr as in above case.. accuracy drops .. loss explodes\n",
        "# good that there was a back up\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtRBlAZh82fr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "ab43ed2f-4ffa-482f-d514-117d4d44f4a9"
      },
      "source": [
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.421653</td>\n",
              "      <td>1.141120</td>\n",
              "      <td>0.635440</td>\n",
              "      <td>07:58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq6Cz2gcQdHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O5P1MsiIPHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://forums.fast.ai/t/continue-training-an-already-trained-model/34790\n",
        "# learn.fit_one_cycle(2, 1e-2) # Another 2 cycles\n",
        "# with ls of 5e-2 the val loss explodes.. and accuracy drops...\n",
        "# the comment above is not exactly true... the loss exploded.. because the lr was not reduced.. as seen in the cell above"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moT93NzGEsHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('second')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y4ADh44Exr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('second');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF_xqXdy_Pba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "8f8b9a58-8983-4f66-8f31-86956f6225c4"
      },
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7)) # almost half an order lower in range this time"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.189231</td>\n",
              "      <td>1.069476</td>\n",
              "      <td>0.667256</td>\n",
              "      <td>09:14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB1psjN8NLf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('third')# some other steps.. and possibly a callback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDI5k00z_bfQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0668206b-29d8-4746-97da-492cfd031168"
      },
      "source": [
        "learn.load('third')"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (6788 items)\n",
              "x: TextList\n",
              "xxbos \n",
              " \n",
              "  xxup cnn just claimed he bought 104 \" semi - automatic assault rifles \" . xxmaj and \n",
              "  they say xxmaj koresh was n't god - like ... xxmaj he managed to buy or build a \n",
              "  collection of fully - automatic semi - automatic rifles ... xxmaj quite a feat , \n",
              "  i would say . ;-) \n",
              " \n",
              "  xxmaj they 're still making charges of \" sexual abuse \" and such , or course . \n",
              "  xxmaj nobody seems to have noticed that the xxmaj treasury department has nothing \n",
              "  to do with sex crimes . xxmaj or maybe the feds have recently instituted a \n",
              "  xxup tax on sex crimes ... xxmaj yeah , that 's why the xxup batf was there , looking for \n",
              "  unregistered * guns * ( \" this is my weapon , this is my gun , this is for \n",
              "  fighting , this is for ... \" ) . \n",
              " \n",
              " \n",
              "  i also heard that they 're claiming to be cautious because of xxmaj koresh 's \n",
              "  \" heated ammunition stockpile \" . i seem to recall that smokeless powder \n",
              "  tends to decompose at even moderate temperatures . i would be rather \n",
              "  surprised , after a fire of that nature , if * any * of his \" stockpile \" is \n",
              "  xxunk , or xxunk . \n",
              " \n",
              " \n",
              "  i seem to recall that aluminum powder is a common component of \n",
              "  xxunk ... xxmaj the folks on xxunk could probably tell you . \n",
              " \n",
              " \n",
              "  i think * anything * is legal if you have the proper license . xxmaj if he had \n",
              "  a \" xxunk and relics \" permit , i believe he could legally own \n",
              "  xxunk to go with his launcher . \n",
              " \n",
              "  -- \n",
              "  xxmaj charles xxmaj scripter * cescript@phy.mtu.edu \n",
              "  xxmaj dept of xxmaj physics , xxmaj michigan xxmaj tech , xxmaj houghton , xxup mi 49931,xxbos xxmaj hi , \n",
              " \t i am looking for some help in choosing a package \n",
              "  for a high - speed silicon xxup adc ( 100mhz ) currently being \n",
              "  fabricated . xxmaj this is a phd research project and i have to test \n",
              "  the chip at speed on a xxup pcb . i expect to have roughly 100 \n",
              "  packaged circuits and will do xxup dc , low - speed and high - speed \n",
              "  testing using 3 different set - ups for the test chip . \n",
              " \t \n",
              " \t i know for sure that a xxup dip will not work \n",
              "  ( the long lead lines have too high an inductance ) . \n",
              "  xxmaj getting a custom - made package is too expensive , so \n",
              "  i am trying to choose between a xxunk and a \n",
              "  leadless chip carrier . xxmaj the xxunk would be hard \n",
              "  to test since it has to be soldered on to the test \n",
              "  setup and i would spend loads of time soldering \n",
              "  as i kept changing the test chip . xxmaj the leadless chip \n",
              "  carrier sockets also have long lead lines and may \n",
              "  not work at high speeds . \n",
              " \n",
              " \t xxmaj does anyone out there have experience / knowledge \n",
              "  of this field ? i would greatly appreciate help ! xxmaj any ideas / \n",
              "  names of companies manufacturing holders / sockets / packages \n",
              "  would help . \n",
              " \n",
              "  xxup p.s. xxmaj the multi - layer fancy gaas packages seem like a bit \n",
              "  of overkill ( ? ),xxbos \n",
              "  i really do n't understand all this ! i watched on satellite network feeds as \n",
              "  perhaps 90 people died before my eyes , while the two xxmaj xxunk 's fanned the flames , \n",
              "  and the xxup fbi stopped the xxunk at the gate . \n",
              " \n",
              "  xxmaj something was xxup very wrong with that scene . \n",
              " \n",
              "  xxmaj perhaps if i 'd watched xxup rambo movies , i might 've been xxunk to the pain of \n",
              "  fellow humans dying . \n",
              " \n",
              "  xxmaj thank xxup god i still feel . i 'm very sorry for you who do n't . xxmaj for you who think \n",
              "  they got what they deserved . xxmaj can you really believe that ? xxmaj even if xxmaj koresh was \n",
              "  the xxunk mad man they said he was , did the others deserve his fate ? xxmaj if , \n",
              "  in fact , he was mad , was n't that even more reason to believe he duped his \n",
              "  followers , and therefore they were innocent , brainwashed , victims ? xxmaj is there \n",
              "  any xxunk that justifies all that death ? \n",
              " \n",
              "  xxmaj and if not , it is clear that the deaths would not have occured if the xxup batf has \n",
              "  not xxup fucked xxup up initially , and now the xxup fbi got xxunk and pushed xxmaj xxunk over \n",
              "  the edge . \n",
              " \n",
              "  xxmaj and that 's if you buy the latest version of the \" story \" hook , line , and xxunk . \n",
              "  i have believed all along that they could not let them live , the embarrassment \n",
              "  to the xxup batf and the xxup fbi would 've been too severe . \n",
              " \n",
              "  xxmaj remember , this was a suspicion of tax - evasion warrant . xxmaj there were no \n",
              "  witnesses , except the xxup fbi . xxmaj all information filtered through the xxup fbi . xxmaj all they \n",
              "  had to do was allow one remote controlled pool camera be installed near the \n",
              "  building , and the press could 've done their job , and would 've been able to back \n",
              "  the xxup fbi 's story with close up video , while xxunk no risk to the press . \n",
              "  xxmaj unless they did not want the public to see something . xxmaj the complete lack of any \n",
              "  other source of information other than the xxup fbi really causes me concern . \n",
              " \n",
              " \n",
              "  xxmaj sick to my stomach , and getting xxunk from all the xxmaj government apologists \n",
              "  -- \n",
              "  jmd@handheld.com,xxbos xxmaj however , this has nothing to do with motorcycling , unless you consider \n",
              "  the xxmaj xxunk a bike .,xxbos i have a novell xxunk that i will sell for $ 692 which can be upgraded to 3.11 \n",
              "  for $ 460 . xxmaj the novell has complete documentation but no network cards except \n",
              "  the xxup id card . \n",
              " \n",
              " \n",
              "  --\n",
              "y: CategoryList\n",
              "16,12,16,8,6\n",
              "Path: /sample_data2;\n",
              "\n",
              "Valid: LabelList (4526 items)\n",
              "x: TextList\n",
              "xxbos \n",
              " \n",
              " \n",
              "  xxmaj and still we wonder why they stereotype us ...,xxbos \n",
              "  i 've been a member of the xxup nra for several years and recently \" joined \" \n",
              "  xxup hci . i wanted to see what they were up to and paid the minimum ( $ 15 ) \n",
              "  to get a membership . i also sent the xxup nra another $ 120 . \n",
              " ,xxbos i have n't seen much info about how to add an extra internal disk to a \n",
              "  mac . xxmaj we would like to try it , and i wonder if someone had some good \n",
              "  advice . \n",
              " \n",
              "  xxmaj we have a xxmaj mac iicx with the original internal xxmaj quantum 40 xxup mb hard disk , \n",
              "  and an unusable floppy drive . xxmaj we also have a new spare xxmaj connor 40 xxup mb \n",
              "  disk which we would like to use . xxmaj the idea is to replace the broken \n",
              "  floppy drive with the new hard disk , but there seems to be some \n",
              "  problems : \n",
              " \n",
              "  xxmaj the internal xxup scsi cable and power cable inside the cx has only \n",
              "  connectors for one single hard disk drive . \n",
              " \n",
              "  xxmaj if i made a ribbon cable and a power cable with three connectors each \n",
              "  ( 1 for motherboard , 1 for each of the 2 disks ) , would it work ? \n",
              " \n",
              "  xxmaj is the iicx able to supply the extra power to the extra disk ? \n",
              " \n",
              "  xxmaj what about xxunk ? i suppose that i should remove the resistor \n",
              "  packs from the disk that is closest to the motherboard , but leave them \n",
              "  installed in the other disk . \n",
              " \n",
              "  xxmaj the xxup scsi xxup id jumpers should also be changed so that the new disk gets \n",
              "  xxup id # 1 . xxmaj the old one should have xxup id # 0 . \n",
              " \n",
              "  xxmaj it is no problem for us to remove the floppy drive , as we have an \n",
              "  external floppy that we can use if it wo n't boot of the hard disk . \n",
              " \n",
              "  xxmaj thank you ! \n",
              " ,xxbos \t  xxrep 6 ^ what the hell xxunk a ' xxunk ' ? ? ( xxunk ( sp ) ) ? ? \n",
              " ,xxbos a question regarding the xxmaj islamic view towards homosexuality came up in a \n",
              "  discussion group that i participate in , and i 'd like to ask the question here , \n",
              " \n",
              "  \" xxmaj what is the xxmaj islamic view towards homosexuality ? \"\n",
              "y: CategoryList\n",
              "8,16,4,6,19\n",
              "Path: /sample_data2;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(33120, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(33120, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f66ddb72ea0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/sample_data2'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (6788 items)\n",
              "x: TextList\n",
              "xxbos \n",
              " \n",
              "  xxup cnn just claimed he bought 104 \" semi - automatic assault rifles \" . xxmaj and \n",
              "  they say xxmaj koresh was n't god - like ... xxmaj he managed to buy or build a \n",
              "  collection of fully - automatic semi - automatic rifles ... xxmaj quite a feat , \n",
              "  i would say . ;-) \n",
              " \n",
              "  xxmaj they 're still making charges of \" sexual abuse \" and such , or course . \n",
              "  xxmaj nobody seems to have noticed that the xxmaj treasury department has nothing \n",
              "  to do with sex crimes . xxmaj or maybe the feds have recently instituted a \n",
              "  xxup tax on sex crimes ... xxmaj yeah , that 's why the xxup batf was there , looking for \n",
              "  unregistered * guns * ( \" this is my weapon , this is my gun , this is for \n",
              "  fighting , this is for ... \" ) . \n",
              " \n",
              " \n",
              "  i also heard that they 're claiming to be cautious because of xxmaj koresh 's \n",
              "  \" heated ammunition stockpile \" . i seem to recall that smokeless powder \n",
              "  tends to decompose at even moderate temperatures . i would be rather \n",
              "  surprised , after a fire of that nature , if * any * of his \" stockpile \" is \n",
              "  xxunk , or xxunk . \n",
              " \n",
              " \n",
              "  i seem to recall that aluminum powder is a common component of \n",
              "  xxunk ... xxmaj the folks on xxunk could probably tell you . \n",
              " \n",
              " \n",
              "  i think * anything * is legal if you have the proper license . xxmaj if he had \n",
              "  a \" xxunk and relics \" permit , i believe he could legally own \n",
              "  xxunk to go with his launcher . \n",
              " \n",
              "  -- \n",
              "  xxmaj charles xxmaj scripter * cescript@phy.mtu.edu \n",
              "  xxmaj dept of xxmaj physics , xxmaj michigan xxmaj tech , xxmaj houghton , xxup mi 49931,xxbos xxmaj hi , \n",
              " \t i am looking for some help in choosing a package \n",
              "  for a high - speed silicon xxup adc ( 100mhz ) currently being \n",
              "  fabricated . xxmaj this is a phd research project and i have to test \n",
              "  the chip at speed on a xxup pcb . i expect to have roughly 100 \n",
              "  packaged circuits and will do xxup dc , low - speed and high - speed \n",
              "  testing using 3 different set - ups for the test chip . \n",
              " \t \n",
              " \t i know for sure that a xxup dip will not work \n",
              "  ( the long lead lines have too high an inductance ) . \n",
              "  xxmaj getting a custom - made package is too expensive , so \n",
              "  i am trying to choose between a xxunk and a \n",
              "  leadless chip carrier . xxmaj the xxunk would be hard \n",
              "  to test since it has to be soldered on to the test \n",
              "  setup and i would spend loads of time soldering \n",
              "  as i kept changing the test chip . xxmaj the leadless chip \n",
              "  carrier sockets also have long lead lines and may \n",
              "  not work at high speeds . \n",
              " \n",
              " \t xxmaj does anyone out there have experience / knowledge \n",
              "  of this field ? i would greatly appreciate help ! xxmaj any ideas / \n",
              "  names of companies manufacturing holders / sockets / packages \n",
              "  would help . \n",
              " \n",
              "  xxup p.s. xxmaj the multi - layer fancy gaas packages seem like a bit \n",
              "  of overkill ( ? ),xxbos \n",
              "  i really do n't understand all this ! i watched on satellite network feeds as \n",
              "  perhaps 90 people died before my eyes , while the two xxmaj xxunk 's fanned the flames , \n",
              "  and the xxup fbi stopped the xxunk at the gate . \n",
              " \n",
              "  xxmaj something was xxup very wrong with that scene . \n",
              " \n",
              "  xxmaj perhaps if i 'd watched xxup rambo movies , i might 've been xxunk to the pain of \n",
              "  fellow humans dying . \n",
              " \n",
              "  xxmaj thank xxup god i still feel . i 'm very sorry for you who do n't . xxmaj for you who think \n",
              "  they got what they deserved . xxmaj can you really believe that ? xxmaj even if xxmaj koresh was \n",
              "  the xxunk mad man they said he was , did the others deserve his fate ? xxmaj if , \n",
              "  in fact , he was mad , was n't that even more reason to believe he duped his \n",
              "  followers , and therefore they were innocent , brainwashed , victims ? xxmaj is there \n",
              "  any xxunk that justifies all that death ? \n",
              " \n",
              "  xxmaj and if not , it is clear that the deaths would not have occured if the xxup batf has \n",
              "  not xxup fucked xxup up initially , and now the xxup fbi got xxunk and pushed xxmaj xxunk over \n",
              "  the edge . \n",
              " \n",
              "  xxmaj and that 's if you buy the latest version of the \" story \" hook , line , and xxunk . \n",
              "  i have believed all along that they could not let them live , the embarrassment \n",
              "  to the xxup batf and the xxup fbi would 've been too severe . \n",
              " \n",
              "  xxmaj remember , this was a suspicion of tax - evasion warrant . xxmaj there were no \n",
              "  witnesses , except the xxup fbi . xxmaj all information filtered through the xxup fbi . xxmaj all they \n",
              "  had to do was allow one remote controlled pool camera be installed near the \n",
              "  building , and the press could 've done their job , and would 've been able to back \n",
              "  the xxup fbi 's story with close up video , while xxunk no risk to the press . \n",
              "  xxmaj unless they did not want the public to see something . xxmaj the complete lack of any \n",
              "  other source of information other than the xxup fbi really causes me concern . \n",
              " \n",
              " \n",
              "  xxmaj sick to my stomach , and getting xxunk from all the xxmaj government apologists \n",
              "  -- \n",
              "  jmd@handheld.com,xxbos xxmaj however , this has nothing to do with motorcycling , unless you consider \n",
              "  the xxmaj xxunk a bike .,xxbos i have a novell xxunk that i will sell for $ 692 which can be upgraded to 3.11 \n",
              "  for $ 460 . xxmaj the novell has complete documentation but no network cards except \n",
              "  the xxup id card . \n",
              " \n",
              " \n",
              "  --\n",
              "y: CategoryList\n",
              "16,12,16,8,6\n",
              "Path: /sample_data2;\n",
              "\n",
              "Valid: LabelList (4526 items)\n",
              "x: TextList\n",
              "xxbos \n",
              " \n",
              " \n",
              "  xxmaj and still we wonder why they stereotype us ...,xxbos \n",
              "  i 've been a member of the xxup nra for several years and recently \" joined \" \n",
              "  xxup hci . i wanted to see what they were up to and paid the minimum ( $ 15 ) \n",
              "  to get a membership . i also sent the xxup nra another $ 120 . \n",
              " ,xxbos i have n't seen much info about how to add an extra internal disk to a \n",
              "  mac . xxmaj we would like to try it , and i wonder if someone had some good \n",
              "  advice . \n",
              " \n",
              "  xxmaj we have a xxmaj mac iicx with the original internal xxmaj quantum 40 xxup mb hard disk , \n",
              "  and an unusable floppy drive . xxmaj we also have a new spare xxmaj connor 40 xxup mb \n",
              "  disk which we would like to use . xxmaj the idea is to replace the broken \n",
              "  floppy drive with the new hard disk , but there seems to be some \n",
              "  problems : \n",
              " \n",
              "  xxmaj the internal xxup scsi cable and power cable inside the cx has only \n",
              "  connectors for one single hard disk drive . \n",
              " \n",
              "  xxmaj if i made a ribbon cable and a power cable with three connectors each \n",
              "  ( 1 for motherboard , 1 for each of the 2 disks ) , would it work ? \n",
              " \n",
              "  xxmaj is the iicx able to supply the extra power to the extra disk ? \n",
              " \n",
              "  xxmaj what about xxunk ? i suppose that i should remove the resistor \n",
              "  packs from the disk that is closest to the motherboard , but leave them \n",
              "  installed in the other disk . \n",
              " \n",
              "  xxmaj the xxup scsi xxup id jumpers should also be changed so that the new disk gets \n",
              "  xxup id # 1 . xxmaj the old one should have xxup id # 0 . \n",
              " \n",
              "  xxmaj it is no problem for us to remove the floppy drive , as we have an \n",
              "  external floppy that we can use if it wo n't boot of the hard disk . \n",
              " \n",
              "  xxmaj thank you ! \n",
              " ,xxbos \t  xxrep 6 ^ what the hell xxunk a ' xxunk ' ? ? ( xxunk ( sp ) ) ? ? \n",
              " ,xxbos a question regarding the xxmaj islamic view towards homosexuality came up in a \n",
              "  discussion group that i participate in , and i 'd like to ask the question here , \n",
              " \n",
              "  \" xxmaj what is the xxmaj islamic view towards homosexuality ? \"\n",
              "y: CategoryList\n",
              "8,16,4,6,19\n",
              "Path: /sample_data2;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(33120, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(33120, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f66ddb72ea0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/sample_data2'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Embedding(33120, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(33120, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(33120, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(33120, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XLM9mLv_lVn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "4a6497c7-ffc6-4420-9602-037c31bd9f8a"
      },
      "source": [
        "learn.unfreeze() # unfreze all\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.943186</td>\n",
              "      <td>1.033650</td>\n",
              "      <td>0.674547</td>\n",
              "      <td>11:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.779844</td>\n",
              "      <td>1.019502</td>\n",
              "      <td>0.684048</td>\n",
              "      <td>12:11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uL9aBgkBzVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('unfreeze_all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAHYB_TDB2qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('unfreeze_all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDQnAO0G9qc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 60-> 67% improvement in accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZacni0oHD49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# next steps \n",
        "# learn deeper at the LM stage.. and the classifier model too 10 cycles see its effect on the  percentage above\n",
        "# throw in a pd.cross tab for a confusion matrix\n",
        "# and possibly a rank n accuracy matric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfqC2JbVQZLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds, targets = learn.get_preds()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9z4nTJARenO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "48e90591-4cbd-4f42-cfc6-a38346335d9a"
      },
      "source": [
        "predictions = np.argmax(preds, axis = 1)\n",
        "pd.crosstab(predictions, targets)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>147</td>\n",
              "      <td>16</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>160</td>\n",
              "      <td>27</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>22</td>\n",
              "      <td>130</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>28</td>\n",
              "      <td>159</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>181</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>177</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>163</td>\n",
              "      <td>25</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>25</td>\n",
              "      <td>179</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>201</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>195</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>178</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>143</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>165</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>180</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>172</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>108</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0  0    1    2    3    4    5    6   ...   13   14   15   16   17   18  19\n",
              "row_0                                    ...                                  \n",
              "0      97    2    0    1    0    0    0  ...    5    4   13    3   10   14  31\n",
              "1       0  147   16   11    9   27    2  ...    5    6    2    0    0    2   0\n",
              "2       0   24  160   27    9    9    4  ...    0    4    0    2    0    0   0\n",
              "3       0   10   22  130   20    2   10  ...    0    0    0    0    1    0   0\n",
              "4       0    4    9   28  159    4    6  ...    0    3    0    0    1    0   0\n",
              "5       0    9    8    0    2  181    2  ...    0    0    1    0    2    0   0\n",
              "6       0    5    1    5    3    2  177  ...    1    1    2    3    1    0   0\n",
              "7       1    0    1    2    3    0    6  ...    1    4    0    4    0    2   3\n",
              "8      12    4    2    8    2    1    7  ...    7    9    1   12    3    7   6\n",
              "9       5    3    2    1    4    6    3  ...    7   10    2    6    7    4  10\n",
              "10      0    0    0    0    1    0    1  ...    0    0    2    1    2    3   0\n",
              "11      9    9    7    2    5    1    7  ...    8   13    5    7    6   10   3\n",
              "12      0    7    4   15   10    4    5  ...    4    4    0    2    1    0   0\n",
              "13      3    5    3    1    0    0    0  ...  191    1    1    2    5    5   1\n",
              "14      8    4    1    2    0    0    1  ...    4  165    3    3    1    3   4\n",
              "15     30    0    0    0    0    0    0  ...    1    3  180    1    1    2  46\n",
              "16     10    0    0    0    1    0    1  ...    1    1    2  151    4   14  15\n",
              "17      7    0    0    0    1    0    1  ...    1    1    5    4  172    8   9\n",
              "18      3    1    0    3    2    0    1  ...    1    8    5   12    8  108   4\n",
              "19      7    0    0    0    0    0    0  ...    1    0   16    5    1    4  19\n",
              "\n",
              "[20 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke7kR4qBRiql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#np.mean(predictions==targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVDkVyfqJAbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}