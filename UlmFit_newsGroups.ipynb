{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UlmFit_newsGroups.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronykroy/DNN-NLP-and-other-stuff/blob/master/UlmFit_newsGroups.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN-c0d1Wbone",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# based on https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson3-imdb.ipynb\n",
        "# but using newsgroups data..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtoluY-3XIWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3sW77aLXAjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fastai\n",
        "from fastai import *\n",
        "from fastai.text import * \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "import io\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnnjWjFpXKOx",
        "colab_type": "code",
        "outputId": "4408ca96-9a6a-4a0e-f210-cb49b99ab347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZAiOxKjXaPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#type(documents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOx8QifIXd15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({'label':dataset.target, 'text':dataset.data})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imuBy_qwptXg",
        "colab_type": "code",
        "outputId": "738ef8a5-6c68-47d9-ca80-0cd2b3d0fd0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f_JnhOWpmnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32faljnbpvhe",
        "colab_type": "code",
        "outputId": "487736d5-91d3-4c7c-b8d3-edfefb5665eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc8XKu-wae7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_d  = pathlib.PosixPath('/sample_data2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT-VkljhhyV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(path_d):\n",
        "    os.mkdir(path_d) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsFGzqDciaEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.rename({'label':'target','text':text},inplace=True) # renaming cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79_hPn4Bi42l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs=32 # was 48\n",
        "# goal fit as much as possible in memory 32 had no issue\n",
        "# 128 ran out of memory\n",
        "# 64 runs out of memory at final stage of classifier trianing unfreeze"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmzyC55cs97i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_trn, df_test = train_test_split(df, stratify = df['label'], test_size = 0.15, random_state = 11) # test_size is debatable\n",
        "df_trn, df_val = train_test_split(df_trn, stratify = df_trn['label'], test_size = 0.15, random_state = 11) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0pLXmCPB6wi",
        "colab_type": "text"
      },
      "source": [
        "## The language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PYKcu7DXoXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm = TextLMDataBunch.from_df(train_df = df_trn, valid_df = df_val, path = path_d)\n",
        "# whatever means/ method you use to make the lm model's data bunch...\n",
        "# use the same for making the classifier data bunch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPsuE3nwYNu1",
        "colab_type": "code",
        "outputId": "8f5f3f5e-8712-442c-c1fa-1478fb297516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "data_lm.save( path_d/'data_lm.pkl') # saving as a back stop"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CrossEntropyLoss. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyKet0ZatiiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm = load_data( '/sample_data2','data_lm.pkl', bs=bs) # give the path and the file in to which this was saved earlier.. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65QLsCDRYOQw",
        "colab_type": "code",
        "outputId": "5d1aa89d-c31e-4db9-c180-c6a830360a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "data_lm.show_batch()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>squeezed xxmaj titles , here 's a crude \\n  picture : \\n \\n  xxrep 22 = xxmaj figure 1 xxrep 36 = \\n  | \\n  | + xxrep 9 - + + xxrep 9 - + + xxrep 9 = + \\n  | + title a + + title b + + title c + \\n  | + xxrep 24 - + + xxrep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>you ca n't move xxmaj ripken out of the # 3 \\n  spot , why not move the rest of the line up ? ) \\n  xxup would xxup be a xxup good xxup sign : xxmaj glenn xxmaj davis wins comeback player of the year . \\n  xxup would xxup be a xxup bad xxup sign : xxmaj in a tight pennant race , team trades</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>their guilt . \\n \\n  xxmaj that the secretary of state may issue an \" xxunk order \" which \\n  bars someone from ever entering a particular part of the xxmaj united xxmaj kingdom , \\n  such as xxmaj northern xxmaj ireland or xxmaj wales . \\n \\n  xxmaj that the xxup bbc banned xxmaj paul mccartney 's \" xxmaj give xxmaj ireland xxmaj back to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>same time , simultaneously . \\n \\n  xxmaj any help well appreciated . xxbos \\n \\n  &gt; \\t xxmaj the defenition of the xxmaj underdog is a team that has no talent and comes \\n  &gt; out of nowhere to contend . xxmaj the ' 69 xxmaj mets and ' 89 xxmaj orioles are prime examples , \\n  &gt; not the xxmaj cubs . \\n \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>\\n  that file , you have a good chance of getting part of the directory and xxup fat \\n  from the other disk written to the new disk . xxmaj this has always been true , \\n  and has destroyed data under other programs , not just cview . \\n \\n  xxmaj the only thing cview can do to improve the situation is to try not</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb5qlPynYPZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7KaRyQ-YQmI",
        "colab_type": "code",
        "outputId": "97860f83-aa0d-4da3-9071-b8a1d82d053f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9_1I7xaaF42",
        "colab_type": "code",
        "outputId": "1608c101-621d-491f-f836-0c2ce9e8b952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "learn.recorder.plot(skip_end=15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEKCAYAAAAvlUMdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMcY44v8Dpy6dA0XhM+QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOlRvNPFaIVU",
        "colab_type": "code",
        "outputId": "35810d82-5f73-44ff-db0d-adcd3de0f065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "learn.fit_one_cycle(10, 1e-2, moms=(0.8,0.7)) # 1e-2 works fair enough as usual "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.314060</td>\n",
              "      <td>4.089117</td>\n",
              "      <td>0.275545</td>\n",
              "      <td>07:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.823380</td>\n",
              "      <td>4.053627</td>\n",
              "      <td>0.275160</td>\n",
              "      <td>07:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.016834</td>\n",
              "      <td>4.094737</td>\n",
              "      <td>0.272546</td>\n",
              "      <td>07:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.900708</td>\n",
              "      <td>4.091317</td>\n",
              "      <td>0.276083</td>\n",
              "      <td>07:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.806482</td>\n",
              "      <td>4.061261</td>\n",
              "      <td>0.280934</td>\n",
              "      <td>07:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.560475</td>\n",
              "      <td>4.051061</td>\n",
              "      <td>0.282974</td>\n",
              "      <td>07:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.490449</td>\n",
              "      <td>4.017698</td>\n",
              "      <td>0.287846</td>\n",
              "      <td>07:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.256065</td>\n",
              "      <td>4.001022</td>\n",
              "      <td>0.290655</td>\n",
              "      <td>07:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.007553</td>\n",
              "      <td>3.998887</td>\n",
              "      <td>0.292218</td>\n",
              "      <td>07:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.085167</td>\n",
              "      <td>3.997532</td>\n",
              "      <td>0.292788</td>\n",
              "      <td>07:40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wSzjXCo0I5k",
        "colab_type": "text"
      },
      "source": [
        "For a language model the accuracy being low is fine.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkPe_fULup7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('fit_head') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp-9-J7NurlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('fit_head');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pSxeSx-us5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.unfreeze() # unfreeze all layers to compete the training of full model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGLkeVUjwiPm",
        "colab_type": "code",
        "outputId": "c757c7c7-2aae-47ec-f75e-8072e238dc6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "learn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7)) # do a full 10 epochs this time around\n",
        "# unlike the classifier's case.. we are not using a slice here..\n",
        "# because we dont experience the losses going [exploding] up phenomenon here....?"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.019767</td>\n",
              "      <td>3.944939</td>\n",
              "      <td>0.304398</td>\n",
              "      <td>08:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.998919</td>\n",
              "      <td>3.892788</td>\n",
              "      <td>0.313871</td>\n",
              "      <td>08:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.811089</td>\n",
              "      <td>3.892653</td>\n",
              "      <td>0.315618</td>\n",
              "      <td>08:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.751428</td>\n",
              "      <td>3.882148</td>\n",
              "      <td>0.324984</td>\n",
              "      <td>08:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.569558</td>\n",
              "      <td>3.915074</td>\n",
              "      <td>0.328590</td>\n",
              "      <td>08:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.389743</td>\n",
              "      <td>3.949040</td>\n",
              "      <td>0.329886</td>\n",
              "      <td>08:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.091447</td>\n",
              "      <td>3.995689</td>\n",
              "      <td>0.331241</td>\n",
              "      <td>08:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.057675</td>\n",
              "      <td>4.044073</td>\n",
              "      <td>0.331113</td>\n",
              "      <td>08:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.858850</td>\n",
              "      <td>4.089037</td>\n",
              "      <td>0.331706</td>\n",
              "      <td>08:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.893411</td>\n",
              "      <td>4.108246</td>\n",
              "      <td>0.330950</td>\n",
              "      <td>08:47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09iBIDOHwqnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('fine_tuned') # save # stop gap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQFzVeb4wsIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('fine_tuned');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWjoRXTAwtfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = \"and today\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2 # test how good the model is at predicting the next few words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFpqJFomwySW",
        "colab_type": "code",
        "outputId": "756aeab3-0790-4508-a2ee-1e3fbfbe1ef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "and today i bought a ' 90 MB Hd for sale . It came with a \n",
            "  brand new Apple wb - only card . It comes with a HD 's \n",
            "  Quantum LPS\n",
            "and today 's SATURDAY Night Live SHOWS - i said that when \n",
            "  the day is first i will see the demonstration . \n",
            "  If you want to wait , i 'll have the day . xxbos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQhObiecwzyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save_encoder('fine_tuned_enc') # we need the encoder in particular.. not exactly the model that predicts next word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOzGi3dcX6YN",
        "colab_type": "text"
      },
      "source": [
        "## The Actual Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyjOp2IzE0MB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifier model data\n",
        "#data_clas = TextClasDataBunch.from_df(path = path_d, train_df = df_trn, valid_df = df_val, vocab=data_lm.train_ds.vocab, bs=bs)\n",
        "data_clas = TextClasDataBunch.from_df(path = path_d, train_df = df_trn, valid_df = df_val, test_df=df_test, vocab=data_lm.train_ds.vocab, bs=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt9KG8sSEnjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5) # the actual classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuBQhtt0FZr0",
        "colab_type": "code",
        "outputId": "2003ee9b-3dcb-4264-bc2d-c314af40bc3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learn.load_encoder('fine_tuned_enc')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (8173 items)\n",
              "x: TextList\n",
              "xxbos i consider xxup twm - style xxmaj squeezed xxmaj titles xxunk in a window \n",
              "  manager . i like to have two tall xterm windows visible at the same \n",
              "  time , with no overlap ; and since two windows are n't enough , i have \n",
              "  other xterm windows underneath them , with exactly the same positioning . \n",
              " \n",
              "  xxmaj in case you 're not familiar with xxmaj squeezed xxmaj titles , here 's a crude \n",
              "  picture : \n",
              " \n",
              "  xxrep 22 = xxmaj figure 1 xxrep 36 = \n",
              "  | \n",
              "  | + xxrep 9 - + + xxrep 9 - + + xxrep 9 = + \n",
              "  | + title a + + title b + + title c + \n",
              "  | + xxrep 24 - + + xxrep 30 - + \n",
              "  | + this is the + + window b hides window c , but + \n",
              "  | + body of the + + you can still see c 's title + \n",
              "  | + window , window a + + which is squeezed right . + \n",
              "  | + xxrep 24 - + + xxrep 30 - + \n",
              "  | \n",
              "  xxrep 22 = xxmaj figure 1 xxrep 36 = \n",
              " \n",
              "  xxmaj squeezed titles allow me to have about 5 such windows in each stack , \n",
              "  with easy access ; and 3 per stack is usually more than i really \n",
              "  need , since i also insist on having a virtual xxup wm . \n",
              " \n",
              "  xxmaj the only problem is that the title location is static , that is , it \n",
              "  is configured in .twmrc , and in order to change it you have to edit \n",
              "  that file and restart the window manager . xxmaj doing so is cumbersome and \n",
              "  time - consuming . \n",
              " \n",
              "  xxmaj therefore , i have implemented xxunk { left , center , right } \n",
              "  functions in my own copy of vtwm ; the idea being that with one click \n",
              "  of a button , you can change this : \n",
              " \n",
              "  + xxrep 9 - + \n",
              "  + title a + \n",
              "  + xxrep 24 - + \n",
              "  + this is the + \n",
              "  + body of the + \n",
              "  + window , window a + \n",
              "  + xxrep 24 - + \n",
              " \n",
              "  to this : \n",
              " \n",
              "  + xxrep 9 - + \n",
              "  + title a + \n",
              "  + xxrep 24 - + \n",
              "  + this is the + \n",
              "  + body of the + \n",
              "  + window , window a + \n",
              "  + xxrep 24 - + \n",
              " \n",
              "  xxrep 15 = \n",
              " \n",
              "  xxmaj okay . xxmaj so far , so good . xxmaj now , how the heck do i get them to put this \n",
              "  into the next \" official \" twm , and the next tvtwm , and the next vtwm , \n",
              "  and the next ctwm ? xxmaj and the next xxunk that i never heard of ? \n",
              " \n",
              "  xxmaj one way would be to post , in comp.windows.x , a description of this \n",
              "  enhancement , together with an explanation of why i think it is a \n",
              "  xxmaj very xxmaj good xxmaj thing , and hope that someone reads it . :-) \n",
              " \n",
              "  xxmaj in case it is n't already clear why i think it 's a xxmaj very xxmaj good xxmaj thing , \n",
              "  look back up at xxmaj figure 1 , and picture window a moved over on top of \n",
              "  windows b and c ; now window a 's title hides xxmaj window b 's title ; \n",
              "  but when you hit f.squeezecenter , the result is : \n",
              " \n",
              "  + xxrep 9 = + + xxrep 9 - + + xxrep 9 = + \n",
              "  + title b + + title a + + title c + \n",
              "  + xxrep 37 - + \n",
              "  + this is the body of the window , + \n",
              "  + window a , which is on top . + \n",
              "  + xxrep 37 - + \n",
              " \n",
              "  xxrep 19 = \n",
              " \n",
              "  xxmaj the rest of this posting explains how to implement it , based on my \n",
              "  xxup x11r4 copy of xxunk ; it 's just a sketch because posting the \n",
              "  full diffs would be too long . \n",
              " \n",
              "  xxmaj the key to this enhancement is to add the following lines in the \n",
              "  xxunk ( ) routine in xxunk : \n",
              " \n",
              "  # ifdef xxup shape \n",
              " \t case xxup f_squeezeleft : \n",
              " \t { \t static xxunk left_squeeze = { xxup xxunk , 0 , 0 } ; \n",
              " \t\t if ( xxunk ( context , xxunk , xxunk ) ) \n",
              " \t\t  return xxup true ; \n",
              " \n",
              " \t\t xxunk = & left_squeeze ; \n",
              " \t\t xxunk ( xxunk ) ; \n",
              " \t\t break ; \n",
              " \t } \n",
              " \t  xxrep 4 . and similarly for xxunk ( xxup xxunk ) and \n",
              " \t xxunk ( xxup xxunk ) ... \n",
              "  # endif \n",
              " \n",
              "  ( xxmaj of course , you also have to define xxup f_squeezeleft in xxunk \n",
              "  and add \n",
              "  { \" xxunk \" , xxup xxunk , xxup f_squeezeleft } , \n",
              "  ... and so forth ... \n",
              "  to xxunk \n",
              " \n",
              "  xxmaj in order to use these functions , add something like the \n",
              "  following to your .twmrc file : \n",
              " \n",
              "  xxmaj xxunk = m | s : xxunk : f.squeezecenter \n",
              " \n",
              "  xxrep 17 = \n",
              " \n",
              "  xxmaj about a year ago , i posted this , but our news was broken and i \n",
              "  * think * it did n't get out . \n",
              " \n",
              "  xxmaj since then , \" blast \" has appeared in comp.sources.x , xxmaj volume 19 , \n",
              "  xxmaj issue 41 ; you could use blast to achieve a similar effect , by \n",
              "  xxunk away at an mwm - style wide title . xxmaj better to have a \n",
              "  twm - style window manager , i think . \n",
              " \n",
              "  -- \n",
              "  xxmaj ralph xxmaj xxunk ( xxup fm ) , \n",
              "  xxunk xxunk,xxbos \n",
              "  xxmaj is this an advantage to xxup ms xxmaj windows or to xxmaj xt ? i used to think it \n",
              "  was a big advantage for xxmaj xt , but i am not at all sure anymore ... \n",
              " \n",
              "  xxrep 73 - \n",
              "  xxmaj david xxmaj smyth \t\t\t\t david@jpl-devvax.jpl.nasa.gov \n",
              "  xxmaj senior xxmaj software xxmaj engineer , \t\t ( 818)306 - 6463 ( temp ! do xxup not use v - mail ) \n",
              "  x and xxmaj object xxmaj guru . \t\t\t tempory office : 525 / xxup b70 \n",
              "  xxmaj jet xxmaj propulsion xxmaj lab , m / s 525 - 3660 4800 xxmaj oak xxmaj grove xxmaj drive , xxmaj pasadena , xxup ca 91109 \n",
              "  xxrep 73 - \n",
              "  xxmaj what 's the earliest possible date you ca n't prove it wo n't be done by ? \n",
              " \t\t\t\t\t - xxmaj tom demarco,xxbos \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              "  xxmaj bzzt . \n",
              "  xxmaj the manta was a two - door sedan in the xxup us . \n",
              "  xxmaj it had a 1900 engine . \n",
              "  xxmaj was sometimes referred to as an xxmaj opel 1900 . \n",
              "  xxmaj manta 's are also ve hot and fun cars too . \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " ,xxbos \n",
              "  @===@ @===@ \n",
              "  # # # xxmaj mark xxmaj xxunk xxup xxunk xxmaj programs # # # \n",
              "  # # # xxunk xxmaj university of xxmaj georgia # # # \n",
              "  # # # xxmaj athens , xxmaj georgia 30602 # # # \n",
              "  @===@ @===@ \n",
              " \n",
              " \n",
              " ,xxbos \n",
              "  [ xxmaj excellent discussion of xxup dc - x landing techniques by xxmaj henry deleted ] \n",
              " \n",
              " \n",
              "  xxmaj the xxup dc - x will not take of horizontally . xxmaj it takes of vertically . \n",
              " \n",
              " \n",
              "  xxmaj for several reasons . xxmaj vertical landings do n't require miles of runway and limit \n",
              "  noise pollution . xxmaj they do n't require wheels or wings . xxmaj just turn on the engines \n",
              "  and touch down . xxmaj of course , as xxmaj henry pointed out , xxunk landings are n't quite \n",
              "  that simple . \n",
              " \n",
              " \n",
              "  xxmaj well , to be blunt , yes . xxmaj but at least you 're learning . \n",
              " \n",
              " \n",
              "  xxmaj the xxmaj soyuz vehicles use parachutes for the descent and then fire small rockets \n",
              "  just before they hit the ground . xxmaj parachutes are , however , not especially \n",
              "  practical if you want to reuse something without much effort . xxmaj the landings \n",
              "  are also not very comfortable . xxmaj however , in the words of xxmaj xxunk xxmaj xxunk , \n",
              "  \" i prefer to have bruises , not to sink . \" \n",
              " \n",
              " \n",
              "y: CategoryList\n",
              "5,5,7,2,14\n",
              "Path: /sample_data2;\n",
              "\n",
              "Valid: LabelList (1443 items)\n",
              "x: TextList\n",
              "xxbos \n",
              "  xxup gm , at least , is heading in that direction . xxmaj one of the post - sale \n",
              "  questions they asked me was if i 'd like the choice of a cigarette \n",
              "  liter or an accessory plug , and another whether i 'd like the choice of \n",
              "  an ashtray or a cup holder . \n",
              " \n",
              "  xxmaj the ' 93 xxmaj geo xxmaj xxunk have the cigarette lighter vs accessory plug option \n",
              "  ( which did not exist in the ' 92 i bought ) -- i 'm not sure about the \n",
              "  ash tray vs cup holder . xxmaj it 's a step in the right direction . \n",
              " \n",
              "  xxmaj the ashtray does make a convenient change - holder so it 's not \n",
              "  completely useless .,xxbos \n",
              "  i 've talked with xxmaj mark and he xxunk some literature , though it was n't very xxunk \n",
              "  just a list of routine names : _ xxunk , _ xxunk ... 241 names . \n",
              "  xxmaj there was a xxmaj product xxmaj info sheet that explained some of the package capabilities . \n",
              "  i also found a review in xxmaj april / xxmaj may ' 92 xxunk .,xxbos \n",
              "  xxmaj so now we 're judging the xxmaj qur'an by what 's not in it ? \n",
              " \n",
              "  xxmaj how many xxunk headed arguments am i going to have to wade \n",
              "  through today ? \n",
              " \n",
              " \n",
              "  xxmaj one would hope . \n",
              " \n",
              "  / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ \n",
              " \n",
              "  xxmaj bob xxmaj beauchaine bobbe@vice . xxup ico.tek.com \n",
              " \n",
              "  xxmaj they said that xxmaj queens could stay , they blew the xxmaj bronx away , \n",
              "  and sank xxmaj manhattan out at sea .,xxbos xxmaj is there a workaround which will enable me to print to a xxup xxunk from my \n",
              "  xxmaj powerbook 100 ? ( xxmaj actually i 'm going to a 4 m which will have an xxmaj ethernet card in \n",
              "  the localtalk xxunk xxrep 5 r ) . xxmaj is there some hardware which will enable me to \n",
              "  this easily ( kind of plug and play ! ) .,xxbos xxmaj from : \" dan xxunk \" < xxunk > \n",
              " \n",
              " \t  i have xxunk here a bit lately , and though some of the math is \n",
              " \t unknown to me , found it interesting . i thought i would post an article i \n",
              " \t found in the xxmaj saturday , xxmaj april 17 , 1993 xxmaj toronto xxmaj star : \n",
              " \n",
              " \t  ' xxup clipper xxup chip ' to protect privacy \n",
              " \n",
              "  xxmaj politics is of course xxmaj dirty xxmaj pool , old man , and here we have a classic \n",
              "  example : the xxup nsa and the administration have been working on this for \n",
              "  a * long * time , and in parallel with the announcement to us xxunk , we \n",
              "  see they 're hitting the press with propoganda . \n",
              " \n",
              "  xxmaj it 's my bet the big magazines - xxmaj byte , xxmaj scientific xxmaj american , et all - will \n",
              "  be ready to run with a pre - written government - slanted story on this in \n",
              "  the next issue . ( ' xxmaj just keep us some pages spare boys , we 'll give you \n",
              "  the copy in time for the presses ' ) \n",
              " \n",
              "  xxmaj we * must * get big names in the industry to write well argued pieces against \n",
              "  this proposal ( can you call it that when it 's a de facto announcement ? ) and \n",
              "  get them into the big magazines before too much damage is done . \n",
              " \n",
              "  xxmaj it would be well worth folks archiving all the discussions from here since \n",
              "  the day of the announcement to keep all the arguments at our fingertips . i \n",
              "  think between us we could write quite a good piece . \n",
              " \n",
              "  xxmaj now , who among us carries enough clout to guarantee publication ? xxmaj phil ? \n",
              "  xxmaj don xxmaj parker ? xxmaj mitch xxmaj kapor ?\n",
              "y: CategoryList\n",
              "7,1,0,4,11\n",
              "Path: /sample_data2;\n",
              "\n",
              "Test: LabelList (1698 items)\n",
              "x: TextList\n",
              "xxbos i have a xxmaj gateway xxup xxunk with my 3.5 inch floppy as drive xxup a. i \n",
              "  accidentally discovered that if a have a floppy from xxup one particular \n",
              "  box of xxunk in the a drive when i boot up , rather than getting \n",
              "  the \" xxmaj non - system diskette \" message , the machine hangs and the xxup cmos \n",
              "  gets xxunk ( luckily , xxmaj gateway sends a print of the standard \n",
              "  xxup cmos settings with their systems ) . xxmaj this only happens with a box \n",
              "  of pre - formatted xxmaj xxunk disks that i have , no other disks cause this \n",
              "  problem . xxmaj if i re - format one of the xxmaj xxunk disks , the problem goes away . \n",
              "  i did a virus scan ( scan xxunk ) of the disks and found nothing . \n",
              " \n",
              "  xxmaj anyone have any idea what is going on here ? xxmaj hardware problem ? a \n",
              "  virus that ca n't be detected ? xxmaj the system reading in garbage from \n",
              "  the boot sector ? \n",
              " ,xxbos \n",
              " \n",
              "  xxmaj interesting is rigth .. i wonder if they will make a mention of her being an \n",
              "  astronaut in the credits .. i think it might help people connect the future of \n",
              "  space with the present .. xxmaj and give them an idea that we must go into space ..,xxbos \n",
              "  : xxmaj it means that the xxup eff 's public stance is complicated with issues irrelevant \n",
              "  : to the encryption issue per se . xxmaj there may well be people who care about \n",
              "  : the encryption issue who do n't care to associate themselves with the \n",
              "  : network erotica issue ( or may even disagree with the xxup eff 's position ) . \n",
              " \n",
              "  xxmaj perhaps these encryption - only types would defend the digitized porn if it \n",
              "  was posted encrypted ?,xxbos xxmaj there was an article on xxmaj jewish major leaguers in a recent issue of \" xxmaj xxunk \n",
              "  xxmaj fields \" , what used to be the \" xxmaj minnesota xxmaj review of xxmaj baseball \" . xxmaj as i recall , \n",
              "  it had an amazing amount of research , with a long list of players and a \n",
              "  large bibliography .,xxbos jayson stark ( i xxunk that 's him ) fits perfectly in this category . \n",
              " \n",
              "  anyone who writes \" dean palmer has 2 homers - at this pace , he 'll \n",
              "  have 324 home runs ! \" should be shot . \n",
              " \n",
              "  if , at the end of april , he has 11 , and anyone writes \" at this \n",
              "  pace , he 'll have 100 + homers ! \" they xxunk shot too .\n",
              "y: EmptyLabelList\n",
              ",,,,\n",
              "Path: /sample_data2, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(36504, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(36504, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f3215a2eea0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/sample_data2'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (8173 items)\n",
              "x: TextList\n",
              "xxbos i consider xxup twm - style xxmaj squeezed xxmaj titles xxunk in a window \n",
              "  manager . i like to have two tall xterm windows visible at the same \n",
              "  time , with no overlap ; and since two windows are n't enough , i have \n",
              "  other xterm windows underneath them , with exactly the same positioning . \n",
              " \n",
              "  xxmaj in case you 're not familiar with xxmaj squeezed xxmaj titles , here 's a crude \n",
              "  picture : \n",
              " \n",
              "  xxrep 22 = xxmaj figure 1 xxrep 36 = \n",
              "  | \n",
              "  | + xxrep 9 - + + xxrep 9 - + + xxrep 9 = + \n",
              "  | + title a + + title b + + title c + \n",
              "  | + xxrep 24 - + + xxrep 30 - + \n",
              "  | + this is the + + window b hides window c , but + \n",
              "  | + body of the + + you can still see c 's title + \n",
              "  | + window , window a + + which is squeezed right . + \n",
              "  | + xxrep 24 - + + xxrep 30 - + \n",
              "  | \n",
              "  xxrep 22 = xxmaj figure 1 xxrep 36 = \n",
              " \n",
              "  xxmaj squeezed titles allow me to have about 5 such windows in each stack , \n",
              "  with easy access ; and 3 per stack is usually more than i really \n",
              "  need , since i also insist on having a virtual xxup wm . \n",
              " \n",
              "  xxmaj the only problem is that the title location is static , that is , it \n",
              "  is configured in .twmrc , and in order to change it you have to edit \n",
              "  that file and restart the window manager . xxmaj doing so is cumbersome and \n",
              "  time - consuming . \n",
              " \n",
              "  xxmaj therefore , i have implemented xxunk { left , center , right } \n",
              "  functions in my own copy of vtwm ; the idea being that with one click \n",
              "  of a button , you can change this : \n",
              " \n",
              "  + xxrep 9 - + \n",
              "  + title a + \n",
              "  + xxrep 24 - + \n",
              "  + this is the + \n",
              "  + body of the + \n",
              "  + window , window a + \n",
              "  + xxrep 24 - + \n",
              " \n",
              "  to this : \n",
              " \n",
              "  + xxrep 9 - + \n",
              "  + title a + \n",
              "  + xxrep 24 - + \n",
              "  + this is the + \n",
              "  + body of the + \n",
              "  + window , window a + \n",
              "  + xxrep 24 - + \n",
              " \n",
              "  xxrep 15 = \n",
              " \n",
              "  xxmaj okay . xxmaj so far , so good . xxmaj now , how the heck do i get them to put this \n",
              "  into the next \" official \" twm , and the next tvtwm , and the next vtwm , \n",
              "  and the next ctwm ? xxmaj and the next xxunk that i never heard of ? \n",
              " \n",
              "  xxmaj one way would be to post , in comp.windows.x , a description of this \n",
              "  enhancement , together with an explanation of why i think it is a \n",
              "  xxmaj very xxmaj good xxmaj thing , and hope that someone reads it . :-) \n",
              " \n",
              "  xxmaj in case it is n't already clear why i think it 's a xxmaj very xxmaj good xxmaj thing , \n",
              "  look back up at xxmaj figure 1 , and picture window a moved over on top of \n",
              "  windows b and c ; now window a 's title hides xxmaj window b 's title ; \n",
              "  but when you hit f.squeezecenter , the result is : \n",
              " \n",
              "  + xxrep 9 = + + xxrep 9 - + + xxrep 9 = + \n",
              "  + title b + + title a + + title c + \n",
              "  + xxrep 37 - + \n",
              "  + this is the body of the window , + \n",
              "  + window a , which is on top . + \n",
              "  + xxrep 37 - + \n",
              " \n",
              "  xxrep 19 = \n",
              " \n",
              "  xxmaj the rest of this posting explains how to implement it , based on my \n",
              "  xxup x11r4 copy of xxunk ; it 's just a sketch because posting the \n",
              "  full diffs would be too long . \n",
              " \n",
              "  xxmaj the key to this enhancement is to add the following lines in the \n",
              "  xxunk ( ) routine in xxunk : \n",
              " \n",
              "  # ifdef xxup shape \n",
              " \t case xxup f_squeezeleft : \n",
              " \t { \t static xxunk left_squeeze = { xxup xxunk , 0 , 0 } ; \n",
              " \t\t if ( xxunk ( context , xxunk , xxunk ) ) \n",
              " \t\t  return xxup true ; \n",
              " \n",
              " \t\t xxunk = & left_squeeze ; \n",
              " \t\t xxunk ( xxunk ) ; \n",
              " \t\t break ; \n",
              " \t } \n",
              " \t  xxrep 4 . and similarly for xxunk ( xxup xxunk ) and \n",
              " \t xxunk ( xxup xxunk ) ... \n",
              "  # endif \n",
              " \n",
              "  ( xxmaj of course , you also have to define xxup f_squeezeleft in xxunk \n",
              "  and add \n",
              "  { \" xxunk \" , xxup xxunk , xxup f_squeezeleft } , \n",
              "  ... and so forth ... \n",
              "  to xxunk \n",
              " \n",
              "  xxmaj in order to use these functions , add something like the \n",
              "  following to your .twmrc file : \n",
              " \n",
              "  xxmaj xxunk = m | s : xxunk : f.squeezecenter \n",
              " \n",
              "  xxrep 17 = \n",
              " \n",
              "  xxmaj about a year ago , i posted this , but our news was broken and i \n",
              "  * think * it did n't get out . \n",
              " \n",
              "  xxmaj since then , \" blast \" has appeared in comp.sources.x , xxmaj volume 19 , \n",
              "  xxmaj issue 41 ; you could use blast to achieve a similar effect , by \n",
              "  xxunk away at an mwm - style wide title . xxmaj better to have a \n",
              "  twm - style window manager , i think . \n",
              " \n",
              "  -- \n",
              "  xxmaj ralph xxmaj xxunk ( xxup fm ) , \n",
              "  xxunk xxunk,xxbos \n",
              "  xxmaj is this an advantage to xxup ms xxmaj windows or to xxmaj xt ? i used to think it \n",
              "  was a big advantage for xxmaj xt , but i am not at all sure anymore ... \n",
              " \n",
              "  xxrep 73 - \n",
              "  xxmaj david xxmaj smyth \t\t\t\t david@jpl-devvax.jpl.nasa.gov \n",
              "  xxmaj senior xxmaj software xxmaj engineer , \t\t ( 818)306 - 6463 ( temp ! do xxup not use v - mail ) \n",
              "  x and xxmaj object xxmaj guru . \t\t\t tempory office : 525 / xxup b70 \n",
              "  xxmaj jet xxmaj propulsion xxmaj lab , m / s 525 - 3660 4800 xxmaj oak xxmaj grove xxmaj drive , xxmaj pasadena , xxup ca 91109 \n",
              "  xxrep 73 - \n",
              "  xxmaj what 's the earliest possible date you ca n't prove it wo n't be done by ? \n",
              " \t\t\t\t\t - xxmaj tom demarco,xxbos \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              "  xxmaj bzzt . \n",
              "  xxmaj the manta was a two - door sedan in the xxup us . \n",
              "  xxmaj it had a 1900 engine . \n",
              "  xxmaj was sometimes referred to as an xxmaj opel 1900 . \n",
              "  xxmaj manta 's are also ve hot and fun cars too . \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " \n",
              " ,xxbos \n",
              "  @===@ @===@ \n",
              "  # # # xxmaj mark xxmaj xxunk xxup xxunk xxmaj programs # # # \n",
              "  # # # xxunk xxmaj university of xxmaj georgia # # # \n",
              "  # # # xxmaj athens , xxmaj georgia 30602 # # # \n",
              "  @===@ @===@ \n",
              " \n",
              " \n",
              " ,xxbos \n",
              "  [ xxmaj excellent discussion of xxup dc - x landing techniques by xxmaj henry deleted ] \n",
              " \n",
              " \n",
              "  xxmaj the xxup dc - x will not take of horizontally . xxmaj it takes of vertically . \n",
              " \n",
              " \n",
              "  xxmaj for several reasons . xxmaj vertical landings do n't require miles of runway and limit \n",
              "  noise pollution . xxmaj they do n't require wheels or wings . xxmaj just turn on the engines \n",
              "  and touch down . xxmaj of course , as xxmaj henry pointed out , xxunk landings are n't quite \n",
              "  that simple . \n",
              " \n",
              " \n",
              "  xxmaj well , to be blunt , yes . xxmaj but at least you 're learning . \n",
              " \n",
              " \n",
              "  xxmaj the xxmaj soyuz vehicles use parachutes for the descent and then fire small rockets \n",
              "  just before they hit the ground . xxmaj parachutes are , however , not especially \n",
              "  practical if you want to reuse something without much effort . xxmaj the landings \n",
              "  are also not very comfortable . xxmaj however , in the words of xxmaj xxunk xxmaj xxunk , \n",
              "  \" i prefer to have bruises , not to sink . \" \n",
              " \n",
              " \n",
              "y: CategoryList\n",
              "5,5,7,2,14\n",
              "Path: /sample_data2;\n",
              "\n",
              "Valid: LabelList (1443 items)\n",
              "x: TextList\n",
              "xxbos \n",
              "  xxup gm , at least , is heading in that direction . xxmaj one of the post - sale \n",
              "  questions they asked me was if i 'd like the choice of a cigarette \n",
              "  liter or an accessory plug , and another whether i 'd like the choice of \n",
              "  an ashtray or a cup holder . \n",
              " \n",
              "  xxmaj the ' 93 xxmaj geo xxmaj xxunk have the cigarette lighter vs accessory plug option \n",
              "  ( which did not exist in the ' 92 i bought ) -- i 'm not sure about the \n",
              "  ash tray vs cup holder . xxmaj it 's a step in the right direction . \n",
              " \n",
              "  xxmaj the ashtray does make a convenient change - holder so it 's not \n",
              "  completely useless .,xxbos \n",
              "  i 've talked with xxmaj mark and he xxunk some literature , though it was n't very xxunk \n",
              "  just a list of routine names : _ xxunk , _ xxunk ... 241 names . \n",
              "  xxmaj there was a xxmaj product xxmaj info sheet that explained some of the package capabilities . \n",
              "  i also found a review in xxmaj april / xxmaj may ' 92 xxunk .,xxbos \n",
              "  xxmaj so now we 're judging the xxmaj qur'an by what 's not in it ? \n",
              " \n",
              "  xxmaj how many xxunk headed arguments am i going to have to wade \n",
              "  through today ? \n",
              " \n",
              " \n",
              "  xxmaj one would hope . \n",
              " \n",
              "  / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ \n",
              " \n",
              "  xxmaj bob xxmaj beauchaine bobbe@vice . xxup ico.tek.com \n",
              " \n",
              "  xxmaj they said that xxmaj queens could stay , they blew the xxmaj bronx away , \n",
              "  and sank xxmaj manhattan out at sea .,xxbos xxmaj is there a workaround which will enable me to print to a xxup xxunk from my \n",
              "  xxmaj powerbook 100 ? ( xxmaj actually i 'm going to a 4 m which will have an xxmaj ethernet card in \n",
              "  the localtalk xxunk xxrep 5 r ) . xxmaj is there some hardware which will enable me to \n",
              "  this easily ( kind of plug and play ! ) .,xxbos xxmaj from : \" dan xxunk \" < xxunk > \n",
              " \n",
              " \t  i have xxunk here a bit lately , and though some of the math is \n",
              " \t unknown to me , found it interesting . i thought i would post an article i \n",
              " \t found in the xxmaj saturday , xxmaj april 17 , 1993 xxmaj toronto xxmaj star : \n",
              " \n",
              " \t  ' xxup clipper xxup chip ' to protect privacy \n",
              " \n",
              "  xxmaj politics is of course xxmaj dirty xxmaj pool , old man , and here we have a classic \n",
              "  example : the xxup nsa and the administration have been working on this for \n",
              "  a * long * time , and in parallel with the announcement to us xxunk , we \n",
              "  see they 're hitting the press with propoganda . \n",
              " \n",
              "  xxmaj it 's my bet the big magazines - xxmaj byte , xxmaj scientific xxmaj american , et all - will \n",
              "  be ready to run with a pre - written government - slanted story on this in \n",
              "  the next issue . ( ' xxmaj just keep us some pages spare boys , we 'll give you \n",
              "  the copy in time for the presses ' ) \n",
              " \n",
              "  xxmaj we * must * get big names in the industry to write well argued pieces against \n",
              "  this proposal ( can you call it that when it 's a de facto announcement ? ) and \n",
              "  get them into the big magazines before too much damage is done . \n",
              " \n",
              "  xxmaj it would be well worth folks archiving all the discussions from here since \n",
              "  the day of the announcement to keep all the arguments at our fingertips . i \n",
              "  think between us we could write quite a good piece . \n",
              " \n",
              "  xxmaj now , who among us carries enough clout to guarantee publication ? xxmaj phil ? \n",
              "  xxmaj don xxmaj parker ? xxmaj mitch xxmaj kapor ?\n",
              "y: CategoryList\n",
              "7,1,0,4,11\n",
              "Path: /sample_data2;\n",
              "\n",
              "Test: LabelList (1698 items)\n",
              "x: TextList\n",
              "xxbos i have a xxmaj gateway xxup xxunk with my 3.5 inch floppy as drive xxup a. i \n",
              "  accidentally discovered that if a have a floppy from xxup one particular \n",
              "  box of xxunk in the a drive when i boot up , rather than getting \n",
              "  the \" xxmaj non - system diskette \" message , the machine hangs and the xxup cmos \n",
              "  gets xxunk ( luckily , xxmaj gateway sends a print of the standard \n",
              "  xxup cmos settings with their systems ) . xxmaj this only happens with a box \n",
              "  of pre - formatted xxmaj xxunk disks that i have , no other disks cause this \n",
              "  problem . xxmaj if i re - format one of the xxmaj xxunk disks , the problem goes away . \n",
              "  i did a virus scan ( scan xxunk ) of the disks and found nothing . \n",
              " \n",
              "  xxmaj anyone have any idea what is going on here ? xxmaj hardware problem ? a \n",
              "  virus that ca n't be detected ? xxmaj the system reading in garbage from \n",
              "  the boot sector ? \n",
              " ,xxbos \n",
              " \n",
              "  xxmaj interesting is rigth .. i wonder if they will make a mention of her being an \n",
              "  astronaut in the credits .. i think it might help people connect the future of \n",
              "  space with the present .. xxmaj and give them an idea that we must go into space ..,xxbos \n",
              "  : xxmaj it means that the xxup eff 's public stance is complicated with issues irrelevant \n",
              "  : to the encryption issue per se . xxmaj there may well be people who care about \n",
              "  : the encryption issue who do n't care to associate themselves with the \n",
              "  : network erotica issue ( or may even disagree with the xxup eff 's position ) . \n",
              " \n",
              "  xxmaj perhaps these encryption - only types would defend the digitized porn if it \n",
              "  was posted encrypted ?,xxbos xxmaj there was an article on xxmaj jewish major leaguers in a recent issue of \" xxmaj xxunk \n",
              "  xxmaj fields \" , what used to be the \" xxmaj minnesota xxmaj review of xxmaj baseball \" . xxmaj as i recall , \n",
              "  it had an amazing amount of research , with a long list of players and a \n",
              "  large bibliography .,xxbos jayson stark ( i xxunk that 's him ) fits perfectly in this category . \n",
              " \n",
              "  anyone who writes \" dean palmer has 2 homers - at this pace , he 'll \n",
              "  have 324 home runs ! \" should be shot . \n",
              " \n",
              "  if , at the end of april , he has 11 , and anyone writes \" at this \n",
              "  pace , he 'll have 100 + homers ! \" they xxunk shot too .\n",
              "y: EmptyLabelList\n",
              ",,,,\n",
              "Path: /sample_data2, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(36504, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(36504, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f3215a2eea0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/sample_data2'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Embedding(36504, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(36504, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(36504, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(36504, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF9-_oOhGGQD",
        "colab_type": "code",
        "outputId": "81b4e79a-736b-4589-ad09-9a494773faf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atI5lUqzGign",
        "colab_type": "code",
        "outputId": "30918fb4-4cee-42b7-eac7-d51c4d05c786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "learn.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxphm/x+ryjVZf+6IHwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dno8wWgCFZms",
        "colab_type": "code",
        "outputId": "862a5f94-3b88-4eea-89a5-27c35030c340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "learn.fit_one_cycle(3, 1e-2, moms=(0.8,0.7)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.533779</td>\n",
              "      <td>1.331054</td>\n",
              "      <td>0.600832</td>\n",
              "      <td>05:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.410977</td>\n",
              "      <td>1.155092</td>\n",
              "      <td>0.654193</td>\n",
              "      <td>05:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.249894</td>\n",
              "      <td>1.156397</td>\n",
              "      <td>0.650035</td>\n",
              "      <td>04:48</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LcKNYTeIkqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('first')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe2EemEYIluN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('first'); # save and load the state where the last layer alone is what has been retrained.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQiClwYK_2Gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.freeze_to(-2) # unfreeze last 2 layers\n",
        "#learn.fit_one_cycle(2, 1e-2, moms=(0.8,0.7))# and then train them"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtRBlAZh82fr",
        "colab_type": "code",
        "outputId": "080e5ffe-cebc-4750-a03e-aa6a63a92660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "learn.fit_one_cycle(2, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.376572</td>\n",
              "      <td>1.179297</td>\n",
              "      <td>0.632017</td>\n",
              "      <td>05:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.113293</td>\n",
              "      <td>1.097845</td>\n",
              "      <td>0.656272</td>\n",
              "      <td>05:22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O5P1MsiIPHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://forums.fast.ai/t/continue-training-an-already-trained-model/34790\n",
        "# learn.fit_one_cycle(2, 1e-2) # Another 2 cycles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moT93NzGEsHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('second')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y4ADh44Exr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('second');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF_xqXdy_Pba",
        "colab_type": "code",
        "outputId": "17b2f59e-a3c0-4bc0-e326-bf6015b180cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(2, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7)) # almost half an order lower in range this time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.134160</td>\n",
              "      <td>1.056748</td>\n",
              "      <td>0.668053</td>\n",
              "      <td>06:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.820568</td>\n",
              "      <td>1.009615</td>\n",
              "      <td>0.699931</td>\n",
              "      <td>06:19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB1psjN8NLf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('third')# todo: some other steps.. and possibly a callback for saving best model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDI5k00z_bfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('third')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XLM9mLv_lVn",
        "colab_type": "code",
        "outputId": "70e54622-9a21-47c4-8117-17ce6bf1bc1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "learn.unfreeze() # unfreze all\n",
        "learn.fit_one_cycle(3, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.774484</td>\n",
              "      <td>0.977860</td>\n",
              "      <td>0.705475</td>\n",
              "      <td>08:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.749806</td>\n",
              "      <td>0.980315</td>\n",
              "      <td>0.710326</td>\n",
              "      <td>08:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.662601</td>\n",
              "      <td>0.970425</td>\n",
              "      <td>0.717949</td>\n",
              "      <td>07:31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uL9aBgkBzVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save('unfreeze_all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAHYB_TDB2qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.load('unfreeze_all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDQnAO0G9qc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 60-> 70% improvement in accuracy 1 1 2 4 : CYCLES\n",
        "# 3224 -> 70+% # NO EPOCHs 3223"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZacni0oHD49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# next steps "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfqC2JbVQZLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds, targets = learn.get_preds() #gets preds of the validation data set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9z4nTJARenO",
        "colab_type": "code",
        "outputId": "5cb5b1fd-1382-4599-e05f-6f2df6371607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "predictions = np.argmax(preds, axis = 1)\n",
        "pd.crosstab(predictions, targets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>51</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>45</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>55</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>54</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>59</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0  0   1   2   3   4   5   6   7   8   ...  11  12  13  14  15  16  17  18  19\n",
              "row_0                                      ...                                    \n",
              "0      34   0   0   0   0   1   0   2   4  ...   2   1   5   0   8   4   2   2   9\n",
              "1       0  43   5   6   1   5   0   1   0  ...   1   6   1   2   1   0   0   0   0\n",
              "2       0   8  51   9   1   0   4   0   0  ...   2   0   0   0   1   1   0   0   1\n",
              "3       0   3  11  45   5   0   3   0   0  ...   0   5   0   0   0   0   0   0   0\n",
              "4       0   4   2   8  55   1   1   1   1  ...   0   5   0   2   0   2   0   1   0\n",
              "5       2   4   3   1   1  64   2   0   2  ...   2   0   0   1   1   1   1   0   0\n",
              "6       0   0   1   2   1   1  54   3   2  ...   0   0   0   0   0   0   0   0   0\n",
              "7       2   0   1   0   1   0   1  59  12  ...   0   5   1   1   0   2   1   2   1\n",
              "8       4   0   0   0   1   0   3   7  48  ...   0   0   0   1   0   0   0   1   0\n",
              "9       0   2   0   0   0   0   1   0   1  ...   0   1   2   1   0   0   1   0   1\n",
              "10      0   0   0   0   0   1   1   0   0  ...   1   1   0   1   0   0   2   0   0\n",
              "11      2   1   0   0   1   0   0   0   1  ...  60   3   0   3   1   1   2   1   0\n",
              "12      0   3   0   3   6   2   2   1   1  ...   1  40   1   1   0   0   1   0   0\n",
              "13      0   2   1   0   0   1   0   0   0  ...   1   2  64   0   0   1   1   1   0\n",
              "14      0   2   0   1   1   0   2   0   1  ...   3   3   0  59   0   1   0   3   0\n",
              "15      8   1   0   0   0   0   1   0   0  ...   1   1   0   2  61   0   1   0  12\n",
              "16      2   1   0   0   0   0   0   0   0  ...   1   0   0   1   0  50   1   3   4\n",
              "17      1   0   0   0   0   0   0   1   0  ...   1   0   0   0   0   1  58   1   1\n",
              "18      2   0   0   0   0   0   0   1   3  ...   0   2   2   0   2   6   1  44   1\n",
              "19      4   0   0   0   0   0   0   0   0  ...   0   0   0   1   1   0   0   0  18\n",
              "\n",
              "[20 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p20ArapELpPW",
        "colab_type": "code",
        "outputId": "11c2820e-daed-46f6-b21c-a4145bdb3b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum((targets ==predictions.squeeze()).numpy())/df_val.shape[0] # accuracy checkls out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.717948717948718"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTZE7-R_MfEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,preds_t1=preds.topk(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5_49k_gMmrW",
        "colab_type": "code",
        "outputId": "c63a72f3-d16f-4bbc-8727-0831b530236c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds_t1.squeeze()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2,  5, 15,  ...,  4,  4,  4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pmQmlWeLo3i",
        "colab_type": "code",
        "outputId": "fd15a3ea-b8e5-493a-ec02-d1e11fa703e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum((targets == preds_t1.squeeze()).numpy())/df_val.shape[0] # topk checks out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.717948717948718"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EzBfhXGM-Iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,preds_t2 = preds.topk(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCBhhoGXM95e",
        "colab_type": "code",
        "outputId": "ad2df06a-2b20-4dc0-996a-fb5860e38f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "preds_t2[]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2,  5],\n",
              "        [ 5,  2],\n",
              "        [15, 19],\n",
              "        ...,\n",
              "        [ 4, 11],\n",
              "        [ 4, 11],\n",
              "        [ 4, 11]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j03MfkgMNMt7",
        "colab_type": "code",
        "outputId": "5dde4715-dc06-40cf-e585-a819c82d3380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds_t2[:,1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5,  2, 19,  ..., 11, 11, 11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52jFhTW6NQTi",
        "colab_type": "code",
        "outputId": "a10f37b9-c139-4053-98cb-c87b28b395ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum((targets == preds_t2[:,1]).numpy())/df_val.shape[0] # no squeeze here"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11365211365211365"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG5R3qNiNeKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,preds_t3 = preds.topk(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWX6XSXTNiS2",
        "colab_type": "code",
        "outputId": "04c6b34a-0549-41cf-f0de-e0cce1afc61c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum((targets == preds_t3[:,2]).numpy())/df_val.shape[0] # no squeeze here"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.042966042966042964"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2H51PDrNnW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#top 3 accuracy\n",
        "# a contrived way admittedly\n",
        "a = np.sum((targets == preds_t1.squeeze()).numpy())/df_val.shape[0] \n",
        "b = np.sum((targets == preds_t2[:,1]).numpy())/df_val.shape[0] \n",
        "c = np.sum((targets == preds_t3[:,2]).numpy())/df_val.shape[0] \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fswzpo9XOG4u",
        "colab_type": "code",
        "outputId": "05551433-152d-4cd0-8986-3480cb5b7589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a+b+c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8745668745668747"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ECNYFs-yfxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO add the df_test into the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3lOsYyHKrfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting top k values \n",
        "#https://github.com/pytorch/pytorch/issues/22812"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MMwwt7UK22z",
        "colab_type": "text"
      },
      "source": [
        "## Running classifier on df_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUpYAALOLAoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preds, targets = learn.get_preds(learn.data.test_ds)\n",
        "preds, targets = learn.get_preds(DatasetType.Test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oxyzDr0RElr",
        "colab_type": "code",
        "outputId": "c626a22a-b5da-41a7-dcba-73009d07b8a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "preds"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[9.0699e-06, 8.4475e-04, 9.9555e-01,  ..., 1.7419e-06, 1.5014e-06,\n",
              "         3.8732e-07],\n",
              "        [3.0804e-05, 9.0752e-01, 7.9175e-03,  ..., 8.9121e-06, 1.6581e-05,\n",
              "         3.3157e-05],\n",
              "        [5.1888e-05, 9.7126e-01, 1.2015e-02,  ..., 1.6015e-05, 2.8513e-06,\n",
              "         8.0924e-05],\n",
              "        ...,\n",
              "        [5.7998e-02, 4.8351e-02, 4.2174e-02,  ..., 3.4702e-02, 3.1545e-02,\n",
              "         6.4266e-02],\n",
              "        [5.7998e-02, 4.8351e-02, 4.2174e-02,  ..., 3.4702e-02, 3.1545e-02,\n",
              "         6.4266e-02],\n",
              "        [5.7998e-02, 4.8351e-02, 4.2174e-02,  ..., 3.4702e-02, 3.1545e-02,\n",
              "         6.4266e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc-pwEqQRGpz",
        "colab_type": "code",
        "outputId": "4b77548f-45ba-422b-ce49-11b1bae1920e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "targets # predictably the targets is all zeros\\\n",
        "# see link\n",
        "# https://forums.fast.ai/t/how-to-reliably-use-get-preds-on-the-test-set-for-ulmfit/34786/4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0,  ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPc-cEx9PIXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}