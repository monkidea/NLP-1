{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2vec in python numpy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronykroy/NLP/blob/master/Word2vec_in_python_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbKDknJ7hl2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Source :: https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/?source=post_page-----13445eebd281----------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJsKA_GShuBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from collections import defaultdict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMYfjVZoiNlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class word2vec():\n",
        "    def __init__ (self):\n",
        "        self.n = settings['n']\n",
        "        self.eta = settings['learning_rate']\n",
        "        self.epochs = settings['epochs']\n",
        "        self.window = settings['window_size']\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    # GENERATE TRAINING DATA\n",
        "    def generate_training_data(self, settings, corpus): # corpus can be a simple array .. at best an array of strings\n",
        "\n",
        "        # GENERATE WORD COUNTS\n",
        "        word_counts = defaultdict(int) # same as a standard dict The value fields' data type is specified upon initialization here its int\n",
        "        for row in corpus:\n",
        "            for word in row:\n",
        "                word_counts[word] += 1 #populate the word_count dictionary thusly\n",
        "\n",
        "        self.v_count = len(word_counts.keys()) # vocabulary count.. is thus the number of words... \n",
        "\n",
        "        # GENERATE LOOKUP DICTIONARIES\n",
        "        self.words_list = sorted(list(word_counts.keys()),reverse=False) # list of words\n",
        "        self.word_index = dict((word, i) for i, word in enumerate(self.words_list)) # a word to integer mapping\n",
        "        self.index_word = dict((i, word) for i, word in enumerate(self.words_list)) # an integer to word mapping reverse lookup\n",
        "\n",
        "        training_data = []\n",
        "        # CYCLE THROUGH EACH SENTENCE IN CORPUS\n",
        "        for sentence in corpus:\n",
        "            sent_len = len(sentence)\n",
        "\n",
        "            # CYCLE THROUGH EACH WORD IN SENTENCE\n",
        "            for i, word in enumerate(sentence):\n",
        "                \n",
        "                #w_target  = sentence[i]\n",
        "                w_target = self.word2onehot(sentence[i])\n",
        "\n",
        "                # CYCLE THROUGH CONTEXT WINDOW\n",
        "                w_context = []\n",
        "                for j in range(i-self.window, i+self.window+1):\n",
        "                    if j!=i and j<=sent_len-1 and j>=0: # conditions to limit J [the sliding window index] within the sentence length\n",
        "                        w_context.append(self.word2onehot(sentence[j]))\n",
        "                training_data.append([w_target, w_context]) # append Y, X to the training data\n",
        "        return np.array(training_data)\n",
        "\n",
        "\n",
        "    # SOFTMAX ACTIVATION FUNCTION\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum(axis=0)\n",
        "\n",
        "\n",
        "    # CONVERT WORD TO ONE HOT ENCODING\n",
        "    def word2onehot(self, word):\n",
        "        word_vec = [0 for i in range(0, self.v_count)] # an array of 0s\n",
        "        word_index = self.word_index[word] # get the word index\n",
        "        word_vec[word_index] = 1 # & set that to 1\n",
        "        return word_vec # return\n",
        "\n",
        "\n",
        "    # FORWARD PASS\n",
        "    def forward_pass(self, x):\n",
        "        h = np.dot(self.w1.T, x) # Hidden Layer:  w1 was v_count*Embedding_size after transpose Embedding_size*v_count\n",
        "        u = np.dot(self.w2.T, h) # see explanaiton in link at source: w2 is context matrix\n",
        "        y_c = self.softmax(u) # thats it.. this is the network architecture\n",
        "        return y_c, h, u\n",
        "    # notes\n",
        "    # no bias here.. dot is the matrix product of type MxN * NxP = MxP sized matrix\n",
        "    # u = w2T(w1T*X) so basically we are optimizing the weights for the context vectors around a word.. (w2 is the context matrix.. see defn)\n",
        "    #  If you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just select the matrix row corresponding to the “1”.\n",
        "    # output of the hidden layer is just the “word vector” for the input word # source:: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
        "\n",
        "    # two different words have very similar “contexts” (that is, what words are likely to appear around them), \n",
        "    # then our model needs to output very similar results for these two words. \n",
        "    # And one way for the network to output similar context predictions \n",
        "    # for these two words is if the word vectors are similar. \n",
        "    # So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words\n",
        "\n",
        "    # the above is what the training does..\n",
        "\n",
        "    # stemming – the network will likely learn similar word vectors for the words “ant” and “ants” because these should have similar contexts. and thus a first step in stemming\n",
        "\n",
        "    # BACKPROPAGATION\n",
        "    def backprop(self, e, h, x):\n",
        "        dl_dw2 = np.outer(h, e)  \n",
        "        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
        "\n",
        "        # UPDATE WEIGHTS\n",
        "        self.w1 = self.w1 - (self.eta * dl_dw1)\n",
        "        self.w2 = self.w2 - (self.eta * dl_dw2)\n",
        "        pass\n",
        "\n",
        "\n",
        "    # TRAIN W2V model\n",
        "    def train(self, training_data):\n",
        "        # INITIALIZE WEIGHT MATRICES\n",
        "        self.w1 = np.random.uniform(-0.8, 0.8, (self.v_count, self.n))     # embedding matrix # Why is it 0.8 to -0.8, Guess: avoiding exploding and diminishing grads this way..?\n",
        "        self.w2 = np.random.uniform(-0.8, 0.8, (self.n, self.v_count))     # context matrix # MxN M rows and N columns\n",
        "        \n",
        "        # CYCLE THROUGH EACH EPOCH\n",
        "        for i in range(0, self.epochs):\n",
        "\n",
        "            self.loss = 0\n",
        "\n",
        "            # CYCLE THROUGH EACH TRAINING SAMPLE\n",
        "            for w_t, w_c in training_data: # for target and context one hot vecotrs... that were 1* vocab length in size\n",
        "\n",
        "                # FORWARD PASS\n",
        "                y_pred, h, u = self.forward_pass(w_t)\n",
        "                \n",
        "                # CALCULATE ERROR\n",
        "                EI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0)\n",
        "\n",
        "                # BACKPROPAGATION\n",
        "                self.backprop(EI, h, w_t)\n",
        "\n",
        "                # CALCULATE LOSS\n",
        "                self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
        "                #self.loss += -2*np.log(len(w_c)) -np.sum([u[word.index(1)] for word in w_c]) + (len(w_c) * np.log(np.sum(np.exp(u))))\n",
        "                \n",
        "            print ('EPOCH:',i, 'LOSS:', self.loss)\n",
        "        pass\n",
        "\n",
        "\n",
        "    # input a word, returns a vector (if available)\n",
        "    def word_vec(self, word):\n",
        "        w_index = self.word_index[word]\n",
        "        v_w = self.w1[w_index]\n",
        "        return v_w\n",
        "\n",
        "\n",
        "    # input a vector, returns nearest word(s)\n",
        "    def vec_sim(self, vec, top_n): # vecotrs .. similar\n",
        "\n",
        "        # CYCLE THROUGH VOCAB\n",
        "        word_sim = {}\n",
        "        for i in range(self.v_count):\n",
        "            v_w2 = self.w1[i]\n",
        "            theta_num = np.dot(vec, v_w2)\n",
        "            theta_den = np.linalg.norm(vec) * np.linalg.norm(v_w2) # norm is basicaly euclidean dist...\n",
        "            theta = theta_num / theta_den\n",
        "\n",
        "            word = self.index_word[i]\n",
        "            word_sim[word] = theta\n",
        "\n",
        "        #words_sorted = sorted(word_sim.items(), key=lambda(word, sim):sim, reverse=True)\n",
        "        # not sure how the above was supposed to work 2.7 syntax may be..?\n",
        "        words_sorted = sorted(word_sim.items(), key = lambda entry: entry[1] , reverse=True)\n",
        "\n",
        "        for word, sim in words_sorted[:top_n]:\n",
        "            print (word, sim)\n",
        "            \n",
        "        pass\n",
        "\n",
        "    # input word, returns top [n] most similar words\n",
        "    def word_sim(self, word, top_n):\n",
        "        \n",
        "        w1_index = self.word_index[word]\n",
        "        v_w1 = self.w1[w1_index]\n",
        "\n",
        "        # CYCLE THROUGH VOCAB\n",
        "        word_sim = {}\n",
        "        for i in range(self.v_count):\n",
        "            v_w2 = self.w1[i]\n",
        "            theta_num = np.dot(v_w1, v_w2)\n",
        "            theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
        "            theta = theta_num / theta_den\n",
        "\n",
        "            word = self.index_word[i]\n",
        "            word_sim[word] = theta\n",
        "\n",
        "        #words_sorted = sorted(word_sim.items(), key=lambda(word, sim):sim, reverse=True)\n",
        "        # not sure how the above was supposed to work 2.7 syntax may be..?\n",
        "        words_sorted = sorted(word_sim.items(), key = lambda entry: entry[1] , reverse=True)\n",
        "        for word, sim in words_sorted[:top_n]:\n",
        "            print (word, sim)\n",
        "            \n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyrqHmo9iaLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_counts = defaultdict(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r554bOiYnc4d",
        "colab_type": "code",
        "outputId": "7127abbc-cd2f-43d6-ca0e-fb2376d6cbf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "word_counts"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IHPhbYlndqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [['the','quick','brown','fox','jumped','over','the','lazy','dog']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge6dH45znizf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for row in corpus:\n",
        "            for word in row:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADnE5rG0nnH4",
        "colab_type": "code",
        "outputId": "0af63d6f-0fa7-4136-e2f3-1f90b0721c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "word_counts"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'brown': 1,\n",
              "             'dog': 1,\n",
              "             'fox': 1,\n",
              "             'jumped': 1,\n",
              "             'lazy': 1,\n",
              "             'over': 1,\n",
              "             'quick': 1,\n",
              "             'the': 2})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaL03EJ26p_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "settings = {}\n",
        "settings['n'] = 5                   # dimension of word embeddings\n",
        "settings['window_size'] = 2         # context window +/- center word\n",
        "settings['min_count'] = 0           # minimum word count\n",
        "settings['epochs'] = 5000           # number of training epochs\n",
        "settings['neg_samp'] = 10           # number of negative words to use during training\n",
        "settings['learning_rate'] = 0.01    # learning rate\n",
        "np.random.seed(0)                   # set the seed for reproducibility\n",
        "\n",
        "corpus = [['the','quick','brown','fox','jumped','over','the','lazy','dog']]\n",
        "\n",
        "# INITIALIZE W2V MODEL\n",
        "w2v = word2vec()\n",
        "\n",
        "# generate training data\n",
        "training_data = w2v.generate_training_data(settings, corpus)\n",
        "\n",
        "# train word2vec model\n",
        "w2v.train(training_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_momPvD76p7N",
        "colab_type": "code",
        "outputId": "49f1d273-2487-4b6e-e321-8fa73f067d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "w2v.words_list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kstY1AYm7_1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#w2v.word_vec('doge') \n",
        "# nice addl feature: if key error word doesnt exist..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqRCPN7X6p3D",
        "colab_type": "code",
        "outputId": "8ea7c8bb-079e-4c38-fd98-f0668aff42d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "w2v.word_vec('dog')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.98920247,  0.54087254,  0.00658507,  3.04401467, -1.11124155])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qUSDFsL6ppo",
        "colab_type": "code",
        "outputId": "ff3eff46-8371-450f-dae3-e0f83814ed51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "w2v.vec_sim(w2v.word_vec('dog'), 2) \n",
        "# here comes class uses... you can make sure that the word you are searching for is being searched in the w2v model you have trained on the particular porpus\n",
        "# given the limited vocab.. results are a tad disappointing"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog 1.0\n",
            "over 0.6268815024010889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoCKXV-n8xLA",
        "colab_type": "code",
        "outputId": "1b65bb99-9fb4-4f1d-e868-d3fa35806a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "w2v.word_sim('dog', 2) # interesting that over is most closely attached to dog... :)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog 1.0\n",
            "over 0.6268815024010889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vihZRts_7V8H",
        "colab_type": "text"
      },
      "source": [
        "### some debugging..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HoI0D5Dnokw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 5 # say twe want a word embeddings to be 5.. usually its in the range of 100 to a 300 beyon a 300 you experience diminishing returns.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhYSZusfu4va",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v_count = len(word_counts.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6XvSLLUu6Qi",
        "colab_type": "code",
        "outputId": "532ea7dd-53e5-42dc-f587-434f676ceeb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "v_count"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wSI4Lsfu7Ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1 = np.random.uniform(-0.8, 0.8, (v_count, n))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKPsHQ4nvgBD",
        "colab_type": "code",
        "outputId": "9e3671f7-c646-42b5-b5da-f82522f9579a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "w1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.38124162, -0.22493038,  0.36001609,  0.37986577, -0.1982128 ],\n",
              "       [-0.74560557, -0.18165655,  0.47181929,  0.0169721 ,  0.05965399],\n",
              "       [-0.5833557 ,  0.48368682, -0.36368072, -0.63716429,  0.44846639],\n",
              "       [ 0.22461991,  0.37057726, -0.0097097 , -0.17843582,  0.66563951],\n",
              "       [ 0.57894744,  0.6148898 , -0.54854941,  0.64356732, -0.00740357],\n",
              "       [ 0.17540658,  0.63163639, -0.30545024, -0.25976584, -0.37109023],\n",
              "       [-0.42026305,  0.0462256 , -0.79040948, -0.44317374,  0.70064574],\n",
              "       [-0.07958058, -0.43943299,  0.70873324,  0.24438961,  0.41438924]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tirknhIB0tji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# place holder\n",
        "vec = w1[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZQy9aYc05od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GENERATE LOOKUP DICTIONARIES\n",
        "words_list = sorted(list(word_counts.keys()),reverse=False) # list of words\n",
        "word_index = dict((word, i) for i, word in enumerate(words_list)) # a word to integer mapping\n",
        "index_word = dict((i, word) for i, word in enumerate(words_list)) # an integer to word mapping reverse lookup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jem9np_4vjXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # CYCLE THROUGH VOCAB\n",
        "        word_sim = {}\n",
        "        for i in range(v_count):\n",
        "            v_w2 = w1[i]\n",
        "            theta_num = np.dot(vec, v_w2)\n",
        "            theta_den = np.linalg.norm(vec) * np.linalg.norm(v_w2)\n",
        "            theta = theta_num / theta_den\n",
        "\n",
        "            word = index_word[i]\n",
        "            word_sim[word] = theta\n",
        "\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22jXBJ1v1DsR",
        "colab_type": "code",
        "outputId": "6bc5e310-1ce3-47d8-9f54-ff2e84e03c67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "word_sim.items()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('brown', -0.34589990862375336), ('dog', -0.2757259436522549), ('fox', 0.49722030549108004), ('jumped', 0.9999999999999998), ('lazy', 0.2502249726125057), ('over', 0.10907028890938095), ('quick', 0.47871844651122014), ('the', 0.05674245997691591)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkjpPRSq1FgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_sorted = sorted(word_sim.items(), key = lambda entry: entry[1] , reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orsaH-b25kIn",
        "colab_type": "code",
        "outputId": "fd23a755-4899-4743-a57c-e17d6568724c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "words_sorted"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('jumped', 0.9999999999999998),\n",
              " ('fox', 0.49722030549108004),\n",
              " ('quick', 0.47871844651122014),\n",
              " ('lazy', 0.2502249726125057),\n",
              " ('over', 0.10907028890938095),\n",
              " ('the', 0.05674245997691591),\n",
              " ('dog', -0.2757259436522549),\n",
              " ('brown', -0.34589990862375336)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96i2m4Sp0hz6",
        "colab_type": "code",
        "outputId": "b03c1a05-4394-4861-a3e6-448779d0cc6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "  words_sorted = sorted(word_sim.items(), key=lambda(word, sim):sim, reverse=True)\n",
        "\n",
        "        for word, sim in words_sorted[:top_n]:\n",
        "            print word, sim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-28fb1b9d4a0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_sim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'word_sim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZsuJ4JQ0lZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_test = defaultdict(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr8LwkxJ3dxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_test = {'ciao': 17, 'bye': 14, 'hello': 23}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP5cl78h3hPb",
        "colab_type": "code",
        "outputId": "64d4880b-d28a-405e-ecda-32cffb260c2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "d_test.items()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('ciao', 17), ('bye', 14), ('hello', 23)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPA4hFNu3jYw",
        "colab_type": "code",
        "outputId": "d1ef7aee-5cfc-4c45-953b-6f82d6ba7660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sorted(d_test.items())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bye', 14), ('ciao', 17), ('hello', 23)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHtcT9d84PBF",
        "colab_type": "code",
        "outputId": "a085d153-0d6c-431a-bc80-e07a4ffd16f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "dir(d_test.items())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__and__',\n",
              " '__class__',\n",
              " '__contains__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__or__',\n",
              " '__rand__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__ror__',\n",
              " '__rsub__',\n",
              " '__rxor__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__sub__',\n",
              " '__subclasshook__',\n",
              " '__xor__',\n",
              " 'isdisjoint']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBGNiM7n3oCa",
        "colab_type": "code",
        "outputId": "5a6209a3-d461-4e80-e94b-2ebf7b55e151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "sorted(d_test.items(), key=lambda(k,v): v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-a122199ca8df>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sorted(d_test.items(), key=lambda(k,v): v)\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG_0qE7136ts",
        "colab_type": "code",
        "outputId": "e8c0cfd8-3e03-4d91-bdda-b048381a0bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sorted(d_test.items(), key=lambda item :item[1])\n",
        "#sorted(lis, key = lambda i: (i['age'], i['name']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bye', 14), ('ciao', 17), ('hello', 23)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyxsedg848e9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}