{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "pygments_lexer": "ipython2", 
            "file_extension": ".py", 
            "name": "python", 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "execution_count": 1, 
            "cell_type": "code", 
            "source": "# ionosphere dataset\n# small numerical values.. withing the same scale..\n# Good for NN", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": 2, 
            "cell_type": "code", 
            "source": "# SGD LR = LR/[1+ decay * epoch] so that with increasing epoch number.. the LR gets reduced.. ", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": 3, 
            "cell_type": "code", 
            "source": "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data -O ionosphere.csv", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "--2018-02-06 04:38:29--  http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\nResolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\nConnecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 76467 (75K) [text/plain]\nSaving to: \u2018ionosphere.csv\u2019\n\n100%[======================================>] 76,467      --.-K/s   in 0.1s    \n\n2018-02-06 04:38:30 (761 KB/s) - \u2018ionosphere.csv\u2019 saved [76467/76467]\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "execution_count": 4, 
            "cell_type": "code", 
            "source": "# Time Based Learning Rate Decay\nfrom pandas import read_csv\nimport numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom sklearn.preprocessing import LabelEncoder\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# load dataset\ndataframe = read_csv(\"ionosphere.csv\", header=None)\ndataset = dataframe.values\n# split into input (X) and output (Y) variables\nX = dataset[:,0:34].astype(float)\nY = dataset[:,34]", 
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "Using TensorFlow backend.\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "execution_count": 5, 
            "cell_type": "code", 
            "source": "# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(Y)\nY = encoder.transform(Y)\n# create model\nmodel = Sequential()\nmodel.add(Dense(34, input_dim=34, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n# Compile model\nepochs = 50\nlearning_rate = 0.1\ndecay_rate = learning_rate / epochs # define decay\nmomentum = 0.8\nsgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False) # include decay in the model\nmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": 6, 
            "cell_type": "code", 
            "source": "# Fit the model\nmodel.fit(X, Y, validation_split=0.33, epochs=epochs, batch_size=28, verbose=2)", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Train on 235 samples, validate on 116 samples\nEpoch 1/50\n0s - loss: 0.6813 - acc: 0.6468 - val_loss: 0.6374 - val_acc: 0.8621\nEpoch 2/50\n0s - loss: 0.6367 - acc: 0.7319 - val_loss: 0.5288 - val_acc: 0.8276\nEpoch 3/50\n0s - loss: 0.5578 - acc: 0.8213 - val_loss: 0.4743 - val_acc: 0.8534\nEpoch 4/50\n0s - loss: 0.4670 - acc: 0.8383 - val_loss: 0.4407 - val_acc: 0.9224\nEpoch 5/50\n0s - loss: 0.3832 - acc: 0.8681 - val_loss: 0.2767 - val_acc: 0.9483\nEpoch 6/50\n0s - loss: 0.3159 - acc: 0.8809 - val_loss: 0.3918 - val_acc: 0.8879\nEpoch 7/50\n0s - loss: 0.2756 - acc: 0.9106 - val_loss: 0.2287 - val_acc: 0.9483\nEpoch 8/50\n0s - loss: 0.2405 - acc: 0.9106 - val_loss: 0.1451 - val_acc: 0.9569\nEpoch 9/50\n0s - loss: 0.2443 - acc: 0.9106 - val_loss: 0.2134 - val_acc: 0.9569\nEpoch 10/50\n0s - loss: 0.2011 - acc: 0.9149 - val_loss: 0.2581 - val_acc: 0.9224\nEpoch 11/50\n0s - loss: 0.1911 - acc: 0.9234 - val_loss: 0.1929 - val_acc: 0.9397\nEpoch 12/50\n0s - loss: 0.1716 - acc: 0.9404 - val_loss: 0.1150 - val_acc: 0.9655\nEpoch 13/50\n0s - loss: 0.1786 - acc: 0.9362 - val_loss: 0.1037 - val_acc: 0.9741\nEpoch 14/50\n0s - loss: 0.1653 - acc: 0.9362 - val_loss: 0.1644 - val_acc: 0.9655\nEpoch 15/50\n0s - loss: 0.1428 - acc: 0.9489 - val_loss: 0.0964 - val_acc: 0.9828\nEpoch 16/50\n0s - loss: 0.1525 - acc: 0.9447 - val_loss: 0.1784 - val_acc: 0.9483\nEpoch 17/50\n0s - loss: 0.1473 - acc: 0.9489 - val_loss: 0.1483 - val_acc: 0.9655\nEpoch 18/50\n0s - loss: 0.1354 - acc: 0.9489 - val_loss: 0.1220 - val_acc: 0.9741\nEpoch 19/50\n0s - loss: 0.1279 - acc: 0.9489 - val_loss: 0.0915 - val_acc: 0.9914\nEpoch 20/50\n0s - loss: 0.1213 - acc: 0.9660 - val_loss: 0.1113 - val_acc: 0.9914\nEpoch 21/50\n0s - loss: 0.1154 - acc: 0.9574 - val_loss: 0.1048 - val_acc: 0.9914\nEpoch 22/50\n0s - loss: 0.1089 - acc: 0.9574 - val_loss: 0.1079 - val_acc: 0.9914\nEpoch 23/50\n0s - loss: 0.1092 - acc: 0.9617 - val_loss: 0.1065 - val_acc: 0.9914\nEpoch 24/50\n0s - loss: 0.1014 - acc: 0.9660 - val_loss: 0.0753 - val_acc: 0.9828\nEpoch 25/50\n0s - loss: 0.1112 - acc: 0.9617 - val_loss: 0.1096 - val_acc: 0.9914\nEpoch 26/50\n0s - loss: 0.0957 - acc: 0.9617 - val_loss: 0.0857 - val_acc: 0.9914\nEpoch 27/50\n0s - loss: 0.0950 - acc: 0.9702 - val_loss: 0.0890 - val_acc: 0.9914\nEpoch 28/50\n0s - loss: 0.0942 - acc: 0.9745 - val_loss: 0.0895 - val_acc: 0.9914\nEpoch 29/50\n0s - loss: 0.0852 - acc: 0.9787 - val_loss: 0.0881 - val_acc: 0.9914\nEpoch 30/50\n0s - loss: 0.0872 - acc: 0.9745 - val_loss: 0.0887 - val_acc: 0.9914\nEpoch 31/50\n0s - loss: 0.0851 - acc: 0.9745 - val_loss: 0.0831 - val_acc: 0.9914\nEpoch 32/50\n0s - loss: 0.0815 - acc: 0.9787 - val_loss: 0.0857 - val_acc: 0.9914\nEpoch 33/50\n0s - loss: 0.0770 - acc: 0.9830 - val_loss: 0.0861 - val_acc: 0.9914\nEpoch 34/50\n0s - loss: 0.0796 - acc: 0.9787 - val_loss: 0.0953 - val_acc: 0.9914\nEpoch 35/50\n0s - loss: 0.0752 - acc: 0.9830 - val_loss: 0.0751 - val_acc: 0.9828\nEpoch 36/50\n0s - loss: 0.0753 - acc: 0.9830 - val_loss: 0.0772 - val_acc: 0.9828\nEpoch 37/50\n0s - loss: 0.0706 - acc: 0.9830 - val_loss: 0.0862 - val_acc: 0.9914\nEpoch 38/50\n0s - loss: 0.0713 - acc: 0.9787 - val_loss: 0.0718 - val_acc: 0.9828\nEpoch 39/50\n0s - loss: 0.0745 - acc: 0.9787 - val_loss: 0.0863 - val_acc: 0.9914\nEpoch 40/50\n0s - loss: 0.0694 - acc: 0.9830 - val_loss: 0.0730 - val_acc: 0.9828\nEpoch 41/50\n0s - loss: 0.0655 - acc: 0.9787 - val_loss: 0.0832 - val_acc: 0.9914\nEpoch 42/50\n0s - loss: 0.0666 - acc: 0.9787 - val_loss: 0.0844 - val_acc: 0.9914\nEpoch 43/50\n0s - loss: 0.0658 - acc: 0.9830 - val_loss: 0.0754 - val_acc: 0.9914\nEpoch 44/50\n0s - loss: 0.0657 - acc: 0.9787 - val_loss: 0.0682 - val_acc: 0.9828\nEpoch 45/50\n0s - loss: 0.0619 - acc: 0.9830 - val_loss: 0.0906 - val_acc: 0.9914\nEpoch 46/50\n0s - loss: 0.0683 - acc: 0.9830 - val_loss: 0.0698 - val_acc: 0.9914\nEpoch 47/50\n0s - loss: 0.0662 - acc: 0.9872 - val_loss: 0.0616 - val_acc: 0.9828\nEpoch 48/50\n0s - loss: 0.0627 - acc: 0.9830 - val_loss: 0.0803 - val_acc: 0.9914\nEpoch 49/50\n0s - loss: 0.0589 - acc: 0.9830 - val_loss: 0.0770 - val_acc: 0.9914\nEpoch 50/50\n0s - loss: 0.0593 - acc: 0.9830 - val_loss: 0.0631 - val_acc: 0.9828\n"
                }, 
                {
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f6823995b90>"
                    }, 
                    "execution_count": 6, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "execution_count": 7, 
            "cell_type": "code", 
            "source": "# the above is time based LR\n# time as in .. with increasing time.. epoch.. the LR decreases..", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": 11, 
            "cell_type": "code", 
            "source": "#The LearningRateScheduler callback allows us to defne a function to call that takes the epoch number as an argument and returns the LR for use in SGD\nfrom keras.callbacks import LearningRateScheduler\nimport math\n\n\n# learning rate schedule\ndef step_decay(epoch):\n    initial_lrate = 0.1\n    drop = 0.5\n    epochs_drop = 10.0\n    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n    return lrate\n\n# Compile model\n# with new parameters for the sgd\nsgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n# learning schedule callback\nlrate = LearningRateScheduler(step_decay)\ncallbacks_list = [lrate]", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": 12, 
            "cell_type": "code", 
            "source": "# Fit the model\nmodel.fit(X, Y, validation_split=0.33, epochs=50, batch_size=28, callbacks=callbacks_list,verbose=2)", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Train on 235 samples, validate on 116 samples\nEpoch 1/50\n0s - loss: 0.0577 - acc: 0.9830 - val_loss: 0.0944 - val_acc: 0.9914\nEpoch 2/50\n0s - loss: 0.0643 - acc: 0.9787 - val_loss: 0.0690 - val_acc: 0.9914\nEpoch 3/50\n0s - loss: 0.0841 - acc: 0.9702 - val_loss: 0.0525 - val_acc: 0.9828\nEpoch 4/50\n0s - loss: 0.0825 - acc: 0.9745 - val_loss: 0.0525 - val_acc: 0.9828\nEpoch 5/50\n0s - loss: 0.0677 - acc: 0.9787 - val_loss: 0.1549 - val_acc: 0.9569\nEpoch 6/50\n0s - loss: 0.0613 - acc: 0.9830 - val_loss: 0.0602 - val_acc: 0.9914\nEpoch 7/50\n0s - loss: 0.0522 - acc: 0.9915 - val_loss: 0.0712 - val_acc: 0.9914\nEpoch 8/50\n0s - loss: 0.0457 - acc: 0.9957 - val_loss: 0.0831 - val_acc: 0.9741\nEpoch 9/50\n0s - loss: 0.0440 - acc: 0.9957 - val_loss: 0.0591 - val_acc: 0.9914\nEpoch 10/50\n0s - loss: 0.0413 - acc: 0.9915 - val_loss: 0.0679 - val_acc: 0.9914\nEpoch 11/50\n0s - loss: 0.0394 - acc: 0.9915 - val_loss: 0.0796 - val_acc: 0.9828\nEpoch 12/50\n0s - loss: 0.0369 - acc: 0.9915 - val_loss: 0.0642 - val_acc: 0.9914\nEpoch 13/50\n0s - loss: 0.0359 - acc: 0.9957 - val_loss: 0.0712 - val_acc: 0.9914\nEpoch 14/50\n0s - loss: 0.0343 - acc: 0.9957 - val_loss: 0.0711 - val_acc: 0.9914\nEpoch 15/50\n0s - loss: 0.0340 - acc: 0.9915 - val_loss: 0.0686 - val_acc: 0.9914\nEpoch 16/50\n0s - loss: 0.0331 - acc: 0.9915 - val_loss: 0.0656 - val_acc: 0.9914\nEpoch 17/50\n0s - loss: 0.0334 - acc: 0.9957 - val_loss: 0.0664 - val_acc: 0.9914\nEpoch 18/50\n0s - loss: 0.0320 - acc: 0.9915 - val_loss: 0.0686 - val_acc: 0.9914\nEpoch 19/50\n0s - loss: 0.0334 - acc: 0.9957 - val_loss: 0.0647 - val_acc: 0.9914\nEpoch 20/50\n0s - loss: 0.0326 - acc: 0.9915 - val_loss: 0.0873 - val_acc: 0.9828\nEpoch 21/50\n0s - loss: 0.0341 - acc: 0.9957 - val_loss: 0.0629 - val_acc: 0.9914\nEpoch 22/50\n0s - loss: 0.0315 - acc: 0.9957 - val_loss: 0.0699 - val_acc: 0.9914\nEpoch 23/50\n0s - loss: 0.0299 - acc: 0.9957 - val_loss: 0.0705 - val_acc: 0.9914\nEpoch 24/50\n0s - loss: 0.0299 - acc: 0.9957 - val_loss: 0.0677 - val_acc: 0.9914\nEpoch 25/50\n0s - loss: 0.0296 - acc: 0.9957 - val_loss: 0.0667 - val_acc: 0.9914\nEpoch 26/50\n0s - loss: 0.0291 - acc: 0.9957 - val_loss: 0.0717 - val_acc: 0.9914\nEpoch 27/50\n0s - loss: 0.0296 - acc: 0.9957 - val_loss: 0.0732 - val_acc: 0.9828\nEpoch 28/50\n0s - loss: 0.0297 - acc: 0.9957 - val_loss: 0.0661 - val_acc: 0.9914\nEpoch 29/50\n0s - loss: 0.0287 - acc: 0.9957 - val_loss: 0.0712 - val_acc: 0.9914\nEpoch 30/50\n0s - loss: 0.0286 - acc: 0.9957 - val_loss: 0.0733 - val_acc: 0.9828\nEpoch 31/50\n0s - loss: 0.0280 - acc: 0.9957 - val_loss: 0.0694 - val_acc: 0.9914\nEpoch 32/50\n0s - loss: 0.0280 - acc: 0.9957 - val_loss: 0.0668 - val_acc: 0.9914\nEpoch 33/50\n0s - loss: 0.0283 - acc: 0.9957 - val_loss: 0.0661 - val_acc: 0.9914\nEpoch 34/50\n0s - loss: 0.0278 - acc: 0.9957 - val_loss: 0.0689 - val_acc: 0.9914\nEpoch 35/50\n0s - loss: 0.0284 - acc: 0.9957 - val_loss: 0.0728 - val_acc: 0.9828\nEpoch 36/50\n0s - loss: 0.0277 - acc: 0.9957 - val_loss: 0.0677 - val_acc: 0.9914\nEpoch 37/50\n0s - loss: 0.0277 - acc: 0.9957 - val_loss: 0.0684 - val_acc: 0.9914\nEpoch 38/50\n0s - loss: 0.0277 - acc: 0.9957 - val_loss: 0.0676 - val_acc: 0.9914\nEpoch 39/50\n0s - loss: 0.0274 - acc: 0.9957 - val_loss: 0.0699 - val_acc: 0.9914\nEpoch 40/50\n0s - loss: 0.0271 - acc: 0.9957 - val_loss: 0.0706 - val_acc: 0.9914\nEpoch 41/50\n0s - loss: 0.0271 - acc: 0.9957 - val_loss: 0.0714 - val_acc: 0.9828\nEpoch 42/50\n0s - loss: 0.0271 - acc: 0.9957 - val_loss: 0.0708 - val_acc: 0.9914\nEpoch 43/50\n0s - loss: 0.0270 - acc: 0.9957 - val_loss: 0.0702 - val_acc: 0.9914\nEpoch 44/50\n0s - loss: 0.0273 - acc: 0.9957 - val_loss: 0.0683 - val_acc: 0.9914\nEpoch 45/50\n0s - loss: 0.0273 - acc: 0.9957 - val_loss: 0.0702 - val_acc: 0.9914\nEpoch 46/50\n0s - loss: 0.0270 - acc: 0.9957 - val_loss: 0.0687 - val_acc: 0.9914\nEpoch 47/50\n0s - loss: 0.0269 - acc: 0.9957 - val_loss: 0.0685 - val_acc: 0.9914\nEpoch 48/50\n0s - loss: 0.0269 - acc: 0.9957 - val_loss: 0.0687 - val_acc: 0.9914\nEpoch 49/50\n0s - loss: 0.0268 - acc: 0.9957 - val_loss: 0.0689 - val_acc: 0.9914\nEpoch 50/50\n0s - loss: 0.0269 - acc: 0.9957 - val_loss: 0.0707 - val_acc: 0.9828\n"
                }, 
                {
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f681af20d50>"
                    }, 
                    "execution_count": 12, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 1
}