{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_pytorch_NewsGroups_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronykroy/DNN-NLP-and-other-stuff/blob/master/Bert_pytorch_NewsGroups_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjn-wQreO965",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Source # https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "# https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7ackDc8R730",
        "colab_type": "text"
      },
      "source": [
        "V2 an attempt to address the max seq length that fits bert model...  \n",
        "By truncation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm_0tFnVRnhW",
        "colab_type": "code",
        "outputId": "b7ace481-fe28-4bfc-b132-1578b1d98b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqjJWuAqRn6y",
        "colab_type": "code",
        "outputId": "8a1f32a1-62c6-43eb-9ae0-06b1481d7cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 6.5MB/s \n",
            "\u001b[?25hCollecting pytorch-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/ae/b6d18c3f37da5a78e83701469e6153811f4b0ecb3f9387bb3e9a65ca48ee/pytorch_nlp-0.4.1-py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.9MB/s \n",
            "\u001b[?25hCollecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/60/d9782c56ceefa76033a00e1f84cd8c586c75e6e7fea2cd45ee8b46a386c5/regex-2019.08.19-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.253)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.3.0+cu100)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: numpy in /tensorflow-2.0.0/python3.6 (from pytorch-pretrained-bert) (1.17.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (0.25.2)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.253 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.253)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.9.11)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2018.9)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.253->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /tensorflow-2.0.0/python3.6 (from python-dateutil>=2.6.1->pandas->pytorch-nlp) (1.12.0)\n",
            "Installing collected packages: regex, pytorch-pretrained-bert, pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.4.1 pytorch-pretrained-bert-0.6.2 regex-2019.8.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLho0HQ0SLwV",
        "colab_type": "code",
        "outputId": "951a2c3b-edc4-4ee5-f997-895e53b6fafc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io, os , re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeULgcvoSOAw",
        "colab_type": "code",
        "outputId": "4313102f-9a72-4146-d0c2-3d06eea54122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0) \n",
        "# get device name.. as device.. to move to gpu later on"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EovgHX665Tn5",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/bentrevett/pytorch-sentiment-analysis  \n",
        "Other ways of doing it in pure pytorch...   \n",
        "A little dated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEnWljX9STGy",
        "colab_type": "code",
        "outputId": "233fb4e4-5f98-4207-aa10-00a7d8f42d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwdEk4Lrqxwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({'label':dataset.target, 'text':dataset.data})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iMBJc3Hq0xV",
        "colab_type": "code",
        "outputId": "c94bafc5-3356-41c6-bb9f-98bca05d615f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17</td>\n",
              "      <td>Well i'm not sure about the story nad it did s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>17</td>\n",
              "      <td>Although I realize that principle is not one o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>Well, I will have to change the scoring on my ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0     17  Well i'm not sure about the story nad it did s...\n",
              "1      0  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...\n",
              "2     17  Although I realize that principle is not one o...\n",
              "3     11  Notwithstanding all the legitimate fuss about ...\n",
              "4     10  Well, I will have to change the scoring on my ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYdEH2PxS95Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df, df_test = train_test_split(df, stratify = df['label'], test_size = 0.15, random_state = 11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7DLtPs6THNY",
        "colab_type": "code",
        "outputId": "8183e23c-e83f-43f3-b95b-d314477c1596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9616, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycv2PsFWQ2ID",
        "colab_type": "code",
        "outputId": "a5dbb306-bf8c-436c-bbcb-6317f9638505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "df.label.value_counts() # close enuf to be evenly distributed"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10    510\n",
              "15    509\n",
              "8     508\n",
              "9     508\n",
              "11    506\n",
              "7     505\n",
              "13    505\n",
              "14    504\n",
              "5     504\n",
              "2     502\n",
              "3     502\n",
              "12    502\n",
              "6     497\n",
              "1     496\n",
              "4     491\n",
              "17    479\n",
              "16    464\n",
              "0     408\n",
              "18    395\n",
              "19    321\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IVZuoR0Qsas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = df.text.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzHdOP5ITIR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences] # cls -> class , sep -> separator\n",
        "labels = df.label.values # in imdb this sentence column may not necessarily have a sentence.. it can have more than 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7WbCKSVTdc6",
        "colab_type": "code",
        "outputId": "ced5deb0-537c-47e9-da5c-be8ec4095ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# the tokenization step\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent)[:511] for sent in sentences] \n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 933045.57B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'in', '<', 'l', '##sr', '##an', '##6', '##inn', '##14', '##a', '@', 'exodus', '.', 'eng', '.', 'sun', '.', 'com', '>', 'em', '##ars', '##h', '@', 'her', '##nes', '-', 'sun', '.', 'eng', '.', 'sun', '.', 'com', '(', 'eric', 'huh', '?', 'please', 'explain', '.', 'is', 'there', 'a', 'problem', 'because', 'i', 'based', 'my', 'morality', 'on', 'something', 'that', 'could', 'be', 'wrong', '?', 'go', '##sh', ',', 'there', \"'\", 's', 'a', 'heck', 'of', 'a', 'lot', 'of', 'stuff', 'that', 'i', 'believe', 'that', 'could', 'be', 'wrong', ',', 'and', 'that', 'comes', 'from', 'sources', 'that', 'could', 'be', 'wrong', '.', 'what', 'do', 'you', 'base', 'your', 'belief', 'on', 'at', '##hei', '##sm', 'on', '?', 'your', 'knowledge', 'and', 'reasoning', '?', 'couldn', \"'\", 't', 'that', 'be', 'wrong', '?', 'mac', '-', '-', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', 'michael', 'a', '.', 'cobb', '\"', '.', '.', '.', 'and', 'i', 'won', \"'\", 't', 'raise', 'taxes', 'on', 'the', 'middle', 'university', 'of', 'illinois', 'class', 'to', 'pay', 'for', 'my', 'programs', '.', '\"', 'champaign', '-', 'urbana', '-', 'bill', 'clinton', '3rd', 'debate', 'cobb', '@', 'alexia', '.', 'li', '##s', '.', 'ui', '##uc', '.', 'ed', '##u', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea-qt3Rw4Av7",
        "colab_type": "text"
      },
      "source": [
        "Trial:  \n",
        "512 is the max seq length so truncate the inputs herre "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUP-D1AbTh0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 128 \n",
        "# good practice to check what is your longest sentence.. in your training corpus\n",
        "# also check that the longest sentence is not an outlier..\n",
        "# if it is.. then settle for some 90-80th %lie length of the sentence lengths distribution"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8dE3T6ATnL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlHVVdV6HAkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Other options to deal with the max seq length bein 512\n",
        "# how to deal with it \n",
        "# https://github.com/huggingface/transformers/issues/89\n",
        "# sliding window approach\n",
        "# https://github.com/huggingface/transformers/issues/89\n",
        "\n",
        "# TODO: check how the sliding window is done in run_squad.py\n",
        "# for now keep going till the code breaks...\n",
        "# might lead to a much lesser accuracy than whats observed with cola task\n",
        "# cola had one sentence.. per line.. and check if its grammatically coorect or not\n",
        "# alter native approach..\n",
        "\n",
        "# tfidf tokenizer.. top 512 words..?\n",
        "# ditch the cls and sep tokens..?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M00ji3eqG_3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set max to 800 or arbit long ones.. in the args\n",
        "#https://github.com/nlpyang/PreSumm/issues/7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejuby-F7E1GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# the irony :)\n",
        "# sequence for a lot of them is > 512 and then we pad the rest..  :p\n",
        "# from keras pre processing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I1bfnpXE51O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJk9qiFpT6ui",
        "colab_type": "text"
      },
      "source": [
        "BERT requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n",
        "\n",
        ">**input ids:** a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary  \n",
        "**segment mask:** (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence  \n",
        "**attention mask:** (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens \n",
        "**labels(the target):** a single value of 1 or 0. In our task 1 means \"grammatical\" and 0 means \"ungrammatical\" < so basically we are training a gammar nazi :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzg3xbGTVhHv",
        "colab_type": "text"
      },
      "source": [
        "### A walkthrough of how we got to attention masks array/list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8F1fW_AVFOH",
        "colab_type": "code",
        "outputId": "30034e83-3be9-4cb4-d9ca-eb8e408cb64b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentences[1]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] Nintendo 8 bit system, power pad, light gun (zapper), 2 controllers\\n\\nGames: Supermario, duck hunt, power field, and wings.\\n\\nAsking $80.\\n [SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJBgjxIPVaK8",
        "colab_type": "code",
        "outputId": "696645f8-6038-4761-a3e6-81af6c1f883c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(tokenized_texts[1])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'nintendo', '8', 'bit', 'system', ',', 'power', 'pad', ',', 'light', 'gun', '(', 'za', '##pper', ')', ',', '2', 'controllers', 'games', ':', 'super', '##mar', '##io', ',', 'duck', 'hunt', ',', 'power', 'field', ',', 'and', 'wings', '.', 'asking', '$', '80', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLW9qCuvVtrg",
        "colab_type": "code",
        "outputId": "9a457994-7158-4f57-96ce-e68b4486d092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "print(input_ids[1]) # @ this place its already padded to max length"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  101 10022  1022  2978  2291  1010  2373 11687  1010  2422  3282  1006\n",
            " 23564 18620  1007  1010  1016 21257  2399  1024  3565  7849  3695  1010\n",
            "  9457  5690  1010  2373  2492  1010  1998  4777  1012  4851  1002  3770\n",
            "  1012   102     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDB6ltcbY8dn",
        "colab_type": "code",
        "outputId": "f0f31d83-aca5-4f3f-cbb5-9f48db84f5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(input_ids[1])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNPGeRDPZAkz",
        "colab_type": "code",
        "outputId": "322fb5e2-8dc7-4fef-ad8d-d930c111047c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(attention_masks[1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpB_kQwcU3WN",
        "colab_type": "code",
        "outputId": "d4634114-6822-41f8-fda1-5b0e9946cb4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(attention_masks[1])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_JLKPSAU_Eq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, # y bother with input ids here then..?\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6raBAFOTZE2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSgKouw6ZGVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkf71l0DZIgV",
        "colab_type": "code",
        "outputId": "0264d82e-a24a-4097-f72a-05895b93a7be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=20) # change the num labels # confirm at the out_features in output of this cell below\n",
        "model.cuda()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:15<00:00, 26961706.48B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKBxfQvIaBBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Dcw3lMNaQQ6",
        "colab_type": "code",
        "outputId": "112801da-cc58-4b09-dd64-78175a7ab5e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=.1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t_total value of -1 results in schedule not being applied\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbF_JK3naRwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdcLn-8qaawJ",
        "colab_type": "code",
        "outputId": "c1108e23-56ea-4045-b810-9f2873f652dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "%%time\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/4 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.5080443583291394\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|██▌       | 1/4 [07:20<22:00, 440.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.657258064516129\n",
            "Train loss: 0.7576530267831584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 2/4 [14:40<14:40, 440.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.7046370967741935\n",
            "Train loss: 0.46137498352140516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  75%|███████▌  | 3/4 [21:59<07:19, 439.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.7217741935483871\n",
            "Train loss: 0.2929756425293609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch: 100%|██████████| 4/4 [29:19<00:00, 439.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.7368951612903226\n",
            "CPU times: user 17min 48s, sys: 11min 26s, total: 29min 14s\n",
            "Wall time: 29min 19s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAlsVockafkg",
        "colab_type": "code",
        "outputId": "d0e9118d-5cce-4c86-94e7-e51bfc7d9fd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAHwCAYAAAD0Es3SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMaxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMdGW3hZaisg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predicitonon hold out set\n",
        "df = df_test\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.text.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df.label.values\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent)[:511] for sent in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nMPvZp3auzZ",
        "colab_type": "code",
        "outputId": "82f73084-8656-4082-9905-7136e265b3bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "MAX_LEN = 128\n",
        "\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "  \n",
        "batch_size = 32  \n",
        "\n",
        "\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (682 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1480 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2684 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (899 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (882 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (859 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2398 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1760 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1320 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1200 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (941 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1720 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1026 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1333 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1240 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (5294 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (699 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (811 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (781 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (824 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (9075 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (9543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2103 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (941 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (745 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (819 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (778 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (930 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1072 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (5344 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1002 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (898 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (726 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (907 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1263 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (905 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1016 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (15003 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (999 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2311 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1131 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3088 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (694 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3009 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (997 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1952 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (927 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (654 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (844 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1233 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (810 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1012 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (972 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (604 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (908 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (769 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1014 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (886 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2577 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (6844 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (4377 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1014 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1229 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (827 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2463 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3327 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (49356 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1890 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (733 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1911 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (881 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (730 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3699 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (13085 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (945 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (15829 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2676 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (765 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (912 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (737 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1030 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1199 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2815 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (625 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (799 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1291 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1615 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (732 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1043 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1018 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1663 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (724 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2695 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3024 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3484 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (964 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1093 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (745 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1190 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (18980 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3171 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (838 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (695 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1392 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (729 > 512). Running this sequence through BERT will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H4vKmyEayvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "901717rja4Dx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "matthews_set = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  matthews = matthews_corrcoef(true_labels[i],\n",
        "                 np.argmax(predictions[i], axis=1).flatten())\n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuIUsDQBa6Bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TUCRE5Ia-tA",
        "colab_type": "code",
        "outputId": "e909860b-7ffa-49d7-a2b1-31d65d8cd46d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_corrcoef(flat_true_labels, flat_predictions)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7179299294500862"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0Z4O7nFa__f",
        "colab_type": "code",
        "outputId": "c3f9f188-629c-4bdd-a7b8-e6ffbcd9e81d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(flat_predictions)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 3 14 11 ...  8  4  8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgIKmvprgzz_",
        "colab_type": "code",
        "outputId": "fccde75e-a8f4-402b-8a56-91275e800d62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(flat_true_labels)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 14, 11, 9, 9, 2, 12, 3, 9, 17, 17, 9, 9, 4, 4, 14, 4, 2, 15, 11, 11, 16, 1, 5, 13, 6, 5, 0, 6, 13, 1, 0, 2, 10, 16, 6, 5, 18, 9, 12, 10, 18, 3, 12, 2, 5, 14, 15, 13, 11, 13, 4, 5, 7, 2, 4, 10, 4, 7, 6, 5, 14, 8, 0, 17, 16, 15, 9, 12, 6, 11, 8, 12, 6, 13, 18, 1, 11, 12, 1, 8, 11, 9, 0, 1, 9, 2, 18, 12, 13, 6, 9, 16, 4, 0, 1, 8, 4, 12, 3, 17, 15, 2, 9, 11, 17, 16, 9, 13, 18, 11, 10, 9, 11, 15, 14, 17, 9, 10, 12, 11, 18, 8, 2, 10, 17, 2, 8, 10, 10, 1, 18, 9, 18, 0, 4, 11, 12, 7, 2, 11, 17, 3, 10, 14, 5, 19, 9, 3, 3, 12, 0, 15, 5, 14, 14, 14, 10, 13, 10, 14, 19, 15, 19, 8, 3, 8, 16, 6, 12, 3, 13, 18, 10, 16, 2, 5, 1, 2, 19, 14, 19, 2, 12, 17, 13, 6, 18, 8, 7, 5, 2, 11, 7, 4, 14, 8, 4, 5, 10, 12, 3, 11, 17, 1, 10, 4, 1, 19, 19, 15, 17, 2, 2, 9, 11, 15, 19, 1, 3, 5, 18, 12, 18, 7, 3, 11, 19, 6, 18, 15, 15, 15, 5, 5, 8, 14, 11, 13, 14, 15, 17, 13, 9, 4, 13, 14, 9, 4, 12, 16, 8, 5, 16, 8, 1, 2, 15, 19, 4, 8, 12, 7, 2, 18, 4, 7, 9, 0, 10, 13, 2, 10, 3, 18, 11, 0, 16, 7, 7, 15, 18, 13, 3, 15, 16, 17, 4, 17, 14, 2, 7, 14, 8, 13, 17, 7, 12, 9, 15, 14, 16, 7, 0, 8, 12, 12, 10, 13, 6, 6, 19, 17, 6, 5, 2, 9, 18, 12, 0, 0, 15, 19, 12, 15, 11, 5, 4, 5, 0, 4, 9, 7, 13, 11, 19, 7, 4, 9, 10, 2, 16, 4, 13, 12, 19, 15, 16, 19, 11, 8, 1, 9, 0, 18, 12, 13, 1, 3, 8, 16, 12, 6, 5, 7, 17, 0, 1, 16, 13, 0, 7, 7, 13, 14, 15, 4, 0, 3, 18, 6, 16, 10, 14, 8, 5, 11, 13, 18, 8, 0, 17, 6, 7, 16, 2, 1, 10, 17, 17, 9, 6, 1, 19, 4, 17, 8, 17, 15, 1, 3, 15, 10, 13, 15, 4, 2, 12, 13, 4, 18, 15, 16, 13, 5, 9, 2, 15, 2, 11, 13, 9, 14, 7, 19, 16, 11, 4, 2, 6, 12, 16, 11, 0, 6, 13, 15, 9, 16, 14, 0, 7, 17, 7, 3, 6, 19, 10, 6, 8, 18, 14, 5, 8, 16, 6, 11, 1, 0, 17, 13, 9, 15, 7, 17, 9, 19, 11, 3, 11, 3, 5, 2, 12, 13, 17, 7, 7, 1, 17, 12, 13, 3, 14, 6, 11, 8, 9, 12, 6, 12, 2, 1, 6, 9, 9, 0, 16, 12, 18, 4, 13, 18, 3, 12, 0, 5, 14, 0, 18, 8, 14, 9, 6, 8, 10, 7, 12, 5, 19, 11, 11, 8, 1, 15, 8, 12, 6, 1, 7, 6, 11, 1, 2, 17, 12, 15, 7, 5, 3, 18, 6, 14, 5, 19, 2, 18, 19, 15, 1, 15, 13, 12, 12, 11, 0, 1, 9, 9, 16, 14, 13, 7, 16, 6, 10, 7, 3, 0, 11, 16, 6, 12, 13, 19, 15, 6, 16, 10, 0, 6, 3, 4, 17, 2, 3, 14, 0, 10, 13, 13, 4, 6, 7, 9, 1, 2, 8, 6, 4, 10, 1, 3, 15, 5, 19, 8, 16, 18, 17, 5, 17, 13, 7, 8, 1, 13, 0, 0, 6, 3, 12, 7, 15, 14, 11, 0, 14, 10, 13, 13, 5, 16, 6, 9, 13, 9, 3, 1, 13, 5, 10, 1, 14, 10, 2, 13, 8, 8, 9, 11, 7, 12, 5, 6, 2, 0, 11, 2, 2, 4, 2, 8, 18, 16, 14, 16, 7, 11, 3, 3, 14, 13, 10, 6, 5, 5, 10, 1, 12, 6, 12, 7, 9, 15, 2, 5, 4, 7, 7, 7, 2, 3, 11, 3, 15, 8, 15, 2, 2, 10, 2, 16, 10, 15, 14, 4, 11, 19, 13, 8, 8, 19, 18, 7, 17, 15, 8, 5, 5, 15, 14, 14, 16, 7, 4, 4, 16, 5, 7, 3, 19, 15, 6, 0, 10, 14, 12, 17, 12, 10, 15, 3, 1, 8, 12, 7, 4, 15, 8, 1, 7, 5, 3, 4, 8, 9, 16, 4, 0, 2, 12, 10, 5, 5, 11, 14, 13, 6, 17, 16, 5, 16, 0, 0, 0, 3, 13, 13, 14, 3, 13, 11, 14, 14, 17, 2, 17, 13, 7, 4, 11, 9, 10, 1, 6, 18, 10, 14, 10, 16, 8, 19, 9, 0, 11, 2, 16, 10, 12, 0, 15, 7, 10, 17, 18, 14, 17, 9, 5, 7, 8, 5, 7, 5, 1, 18, 17, 5, 6, 13, 14, 2, 6, 4, 17, 5, 5, 13, 2, 1, 6, 8, 17, 1, 3, 4, 7, 15, 9, 12, 3, 6, 1, 11, 2, 2, 8, 7, 14, 19, 0, 2, 18, 4, 1, 17, 14, 0, 7, 7, 0, 8, 6, 6, 3, 0, 10, 10, 8, 8, 6, 13, 7, 3, 18, 17, 9, 5, 5, 12, 12, 13, 17, 16, 0, 18, 7, 7, 12, 18, 15, 2, 1, 17, 2, 0, 17, 16, 6, 8, 6, 9, 3, 18, 18, 10, 1, 16, 3, 9, 11, 9, 4, 10, 7, 5, 17, 17, 4, 3, 3, 18, 4, 7, 11, 10, 10, 6, 18, 18, 0, 10, 11, 13, 7, 0, 11, 1, 12, 5, 3, 11, 10, 4, 11, 18, 14, 13, 9, 6, 6, 18, 3, 10, 18, 8, 12, 14, 14, 12, 19, 1, 9, 17, 0, 11, 19, 10, 9, 7, 1, 0, 5, 19, 8, 2, 11, 1, 15, 8, 5, 12, 6, 17, 8, 19, 11, 1, 7, 17, 1, 15, 18, 17, 1, 4, 4, 12, 18, 9, 16, 12, 3, 13, 5, 13, 0, 16, 9, 8, 12, 12, 1, 6, 16, 16, 11, 5, 6, 11, 11, 8, 3, 14, 17, 15, 17, 10, 10, 14, 8, 4, 14, 13, 6, 3, 9, 15, 9, 2, 7, 13, 1, 12, 9, 18, 4, 4, 4, 4, 7, 14, 13, 14, 9, 16, 4, 4, 1, 13, 0, 4, 18, 9, 17, 12, 10, 16, 18, 17, 16, 10, 7, 12, 17, 15, 3, 17, 7, 16, 10, 19, 0, 7, 6, 1, 15, 11, 16, 3, 15, 9, 13, 13, 1, 17, 4, 7, 17, 18, 0, 17, 11, 8, 13, 15, 16, 7, 0, 15, 14, 15, 2, 18, 16, 10, 12, 8, 12, 15, 11, 1, 8, 8, 2, 12, 7, 13, 19, 10, 15, 14, 9, 7, 12, 11, 5, 18, 9, 5, 10, 0, 14, 17, 18, 2, 15, 11, 9, 0, 17, 7, 12, 9, 2, 3, 1, 16, 17, 6, 5, 13, 1, 2, 19, 15, 9, 4, 10, 0, 17, 5, 5, 19, 7, 17, 10, 13, 6, 15, 2, 12, 16, 15, 16, 1, 1, 2, 5, 14, 3, 15, 12, 17, 11, 8, 1, 10, 8, 19, 14, 17, 14, 2, 15, 17, 15, 18, 11, 7, 15, 8, 7, 3, 19, 17, 10, 15, 9, 6, 6, 4, 6, 5, 11, 11, 16, 1, 6, 5, 3, 15, 2, 14, 4, 5, 2, 9, 18, 11, 14, 1, 14, 16, 19, 19, 5, 10, 3, 12, 19, 18, 8, 9, 6, 3, 15, 18, 10, 8, 17, 5, 1, 7, 13, 8, 8, 2, 1, 19, 4, 19, 10, 6, 6, 18, 9, 2, 3, 16, 10, 12, 13, 1, 18, 2, 13, 0, 10, 18, 13, 4, 16, 4, 8, 14, 5, 8, 1, 3, 6, 16, 19, 16, 8, 14, 16, 7, 19, 4, 7, 4, 10, 14, 12, 13, 2, 11, 12, 2, 10, 6, 11, 15, 14, 12, 4, 11, 15, 17, 5, 7, 1, 3, 12, 6, 6, 15, 4, 15, 6, 10, 9, 7, 4, 14, 3, 12, 14, 15, 16, 2, 0, 2, 15, 0, 12, 12, 7, 16, 10, 11, 0, 15, 16, 12, 4, 17, 13, 9, 10, 13, 11, 8, 2, 14, 15, 14, 16, 1, 14, 3, 14, 18, 4, 1, 1, 12, 14, 5, 3, 15, 3, 17, 6, 19, 1, 13, 13, 1, 1, 9, 8, 14, 2, 6, 5, 16, 2, 3, 2, 19, 0, 14, 6, 6, 15, 12, 3, 17, 3, 1, 17, 1, 11, 5, 5, 8, 3, 3, 4, 1, 13, 19, 8, 11, 1, 9, 19, 6, 7, 2, 5, 11, 0, 8, 13, 7, 9, 0, 11, 8, 15, 3, 4, 12, 11, 4, 14, 14, 11, 0, 15, 10, 14, 8, 16, 5, 4, 16, 16, 16, 1, 1, 14, 5, 11, 14, 18, 16, 9, 8, 12, 15, 10, 4, 17, 17, 9, 7, 9, 8, 13, 5, 17, 18, 3, 0, 8, 3, 7, 11, 10, 13, 1, 10, 13, 3, 2, 15, 16, 19, 4, 4, 17, 4, 3, 14, 6, 19, 16, 5, 0, 11, 4, 13, 13, 18, 12, 2, 8, 17, 9, 15, 19, 6, 2, 9, 11, 2, 10, 0, 15, 5, 0, 2, 17, 3, 5, 14, 12, 2, 14, 18, 5, 0, 1, 1, 14, 8, 7, 1, 1, 2, 3, 9, 6, 2, 10, 5, 13, 14, 10, 16, 16, 13, 15, 8, 5, 9, 12, 18, 10, 11, 15, 13, 16, 9, 4, 10, 17, 7, 10, 5, 3, 3, 3, 12, 15, 1, 11, 16, 7, 16, 13, 6, 3, 11, 2, 8, 10, 0, 5, 9, 16, 11, 17, 3, 3, 10, 19, 7, 7, 9, 5, 3, 1, 12, 5, 6, 15, 10, 4, 6, 4, 19, 1, 5, 5, 18, 9, 19, 10, 3, 6, 14, 7, 18, 17, 8, 4, 8, 4, 0, 6, 9, 8, 3, 10, 11, 3, 2, 11, 2, 2, 9, 4, 4, 1, 14, 6, 18, 8, 10, 8, 4, 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0NpEA0bg5q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQEpDpx-hz0l",
        "colab_type": "code",
        "outputId": "347433a0-e283-470b-aa28-735bd593b320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "results = confusion_matrix(flat_true_labels, flat_predictions) \n",
        "print ('Confusion Matrix :')\n",
        "print(results) "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix :\n",
            "[[44  0  0  0  0  0  0  5  0  1  0  0  0  1  1 12  1  0  2  5]\n",
            " [ 2 65  7  3  2  2  2  2  0  0  0  0  1  0  2  0  0  0  0  0]\n",
            " [ 0  5 62  8  1  3  1  7  0  1  0  0  0  1  0  0  0  0  0  0]\n",
            " [ 0  2  3 62  9  0  4  2  0  0  0  0  5  1  0  0  0  0  0  0]\n",
            " [ 0  1  0 11 60  0  7  4  0  0  0  0  4  0  0  0  0  0  0  0]\n",
            " [ 0  5  5  0  1 74  2  0  0  0  0  1  0  0  1  0  0  0  0  0]\n",
            " [ 0  1  0  1  3  1 78  1  0  1  0  0  0  0  0  1  0  0  1  0]\n",
            " [ 1  0  0  0  1  0  1 75  3  3  0  0  2  0  0  0  1  0  1  1]\n",
            " [ 0  1  0  0  2  0  3 16 59  0  0  0  3  3  1  0  1  0  1  0]\n",
            " [ 1  1  0  0  0  0  0  4  1 76  3  1  0  0  0  1  1  0  0  0]\n",
            " [ 2  0  0  0  0  0  1  6  0  6 71  0  1  0  0  1  0  2  0  0]\n",
            " [ 2  0  2  2  1  0  0  4  0  0  0 66  4  0  0  0  6  0  2  0]\n",
            " [ 0  2  0  5  7  1  2  2  2  0  0  0 65  1  1  0  1  0  0  0]\n",
            " [ 3  2  0  0  1  0  0  4  1  1  0  0  2 68  2  0  1  0  3  1]\n",
            " [ 1  1  1  0  3  1  0  3  1  1  0  0  1  3 68  0  0  0  3  2]\n",
            " [ 5  0  0  0  0  0  0  0  1  0  0  0  0  0  1 72  0  0  1 10]\n",
            " [ 4  1  0  0  0  0  0  5  2  1  2  2  2  0  1  0 56  0  3  3]\n",
            " [ 4  0  0  0  0  0  0  4  0  1  1  0  0  0  1  0  1 71  2  0]\n",
            " [10  1  0  0  0  0  1  5  3  0  4  0  1  2  0  0  3  0 37  3]\n",
            " [ 8  0  0  0  0  0  0  3  0  1  0  1  2  1  2 20  3  1  1 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8LIccUQiNhh",
        "colab_type": "code",
        "outputId": "35413e4f-6ab6-4d02-ef5f-3ee25a402cbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "pd.DataFrame(results) #neater"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>65</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>62</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>62</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>74</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>78</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>75</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>59</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>71</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>66</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>68</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>72</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>71</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>37</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0   1   2   3   4   5   6   7   8   ...  11  12  13  14  15  16  17  18  19\n",
              "0   44   0   0   0   0   0   0   5   0  ...   0   0   1   1  12   1   0   2   5\n",
              "1    2  65   7   3   2   2   2   2   0  ...   0   1   0   2   0   0   0   0   0\n",
              "2    0   5  62   8   1   3   1   7   0  ...   0   0   1   0   0   0   0   0   0\n",
              "3    0   2   3  62   9   0   4   2   0  ...   0   5   1   0   0   0   0   0   0\n",
              "4    0   1   0  11  60   0   7   4   0  ...   0   4   0   0   0   0   0   0   0\n",
              "5    0   5   5   0   1  74   2   0   0  ...   1   0   0   1   0   0   0   0   0\n",
              "6    0   1   0   1   3   1  78   1   0  ...   0   0   0   0   1   0   0   1   0\n",
              "7    1   0   0   0   1   0   1  75   3  ...   0   2   0   0   0   1   0   1   1\n",
              "8    0   1   0   0   2   0   3  16  59  ...   0   3   3   1   0   1   0   1   0\n",
              "9    1   1   0   0   0   0   0   4   1  ...   1   0   0   0   1   1   0   0   0\n",
              "10   2   0   0   0   0   0   1   6   0  ...   0   1   0   0   1   0   2   0   0\n",
              "11   2   0   2   2   1   0   0   4   0  ...  66   4   0   0   0   6   0   2   0\n",
              "12   0   2   0   5   7   1   2   2   2  ...   0  65   1   1   0   1   0   0   0\n",
              "13   3   2   0   0   1   0   0   4   1  ...   0   2  68   2   0   1   0   3   1\n",
              "14   1   1   1   0   3   1   0   3   1  ...   0   1   3  68   0   0   0   3   2\n",
              "15   5   0   0   0   0   0   0   0   1  ...   0   0   0   1  72   0   0   1  10\n",
              "16   4   1   0   0   0   0   0   5   2  ...   2   2   0   1   0  56   0   3   3\n",
              "17   4   0   0   0   0   0   0   4   0  ...   0   0   0   1   0   1  71   2   0\n",
              "18  10   1   0   0   0   0   1   5   3  ...   0   1   2   0   0   3   0  37   3\n",
              "19   8   0   0   0   0   0   0   3   0  ...   1   2   1   2  20   3   1   1  13\n",
              "\n",
              "[20 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzRXIieWh1Rm",
        "colab_type": "code",
        "outputId": "dc1c2b81-3e43-411a-c845-1d06fdab0f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print ('Accuracy Score :',accuracy_score(flat_true_labels, flat_predictions) )"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score : 0.7314487632508834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl17W8bx_zB1",
        "colab_type": "text"
      },
      "source": [
        "Truncation leads to a marginal drop in accuracy after the 2nd place  after decimal point.. which is ok.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryCG7w77iCiV",
        "colab_type": "code",
        "outputId": "ca1c2c09-96f3-42cd-a95e-be2b9ea7895e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "print ('Report : ')\n",
        "print (classification_report(flat_true_labels, flat_predictions) )"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.61      0.55        72\n",
            "           1       0.74      0.74      0.74        88\n",
            "           2       0.78      0.70      0.73        89\n",
            "           3       0.67      0.70      0.69        88\n",
            "           4       0.66      0.69      0.67        87\n",
            "           5       0.90      0.83      0.87        89\n",
            "           6       0.76      0.89      0.82        88\n",
            "           7       0.49      0.84      0.62        89\n",
            "           8       0.81      0.66      0.72        90\n",
            "           9       0.82      0.85      0.84        89\n",
            "          10       0.88      0.79      0.83        90\n",
            "          11       0.93      0.74      0.82        89\n",
            "          12       0.70      0.73      0.71        89\n",
            "          13       0.84      0.76      0.80        89\n",
            "          14       0.84      0.76      0.80        89\n",
            "          15       0.67      0.80      0.73        90\n",
            "          16       0.75      0.68      0.71        82\n",
            "          17       0.96      0.84      0.89        85\n",
            "          18       0.65      0.53      0.58        70\n",
            "          19       0.34      0.23      0.28        56\n",
            "\n",
            "    accuracy                           0.73      1698\n",
            "   macro avg       0.73      0.72      0.72      1698\n",
            "weighted avg       0.75      0.73      0.73      1698\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYom6v4miFqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
